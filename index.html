<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
 
 <meta name="description" content="Senior Tech Lead (Meta FAIR) and Affiliate Associate Professor (University of Washington)">
 <meta name="og:description" content="Senior Research Scientist Manager (Meta FAIR) and Affiliate Associate Professor (University of Washington)">
 <meta name="robots" content="all">
 <title>Roozbeh Mottaghi's Webpage</title>
 <link rel="stylesheet" href="./misc/mystyle.css">
 <link rel="stylesheet" href="./misc/pygments.css">
</head>

<body>

<!-- Header -->
<header>
<h2><a href="index.html">Roozbeh Mottaghi</a></h2>
<nav>
<ul>
 <li><a href="index.html">Home</a></li>
  <li><a href="#pubs">Publications</a></li>
  <li><a href="misc/resume.pdf">CV</a></li>
</ul>
</nav>
</header>

<!-- Affiliations + Contact -->
<section style="height: 250px;">
<div class ="profile">
	<img src="./images/profile_roozbehM.jpg" align="left" width="230" style="margin-top:5px; margin-right:81px" alt="me">
	<div>
	<b>Roozbeh Mottaghi</b>
	<br><br>
	Senior Tech Lead <br>
	<a href="https://ai.meta.com/">Meta</a> Fundamental AI Research (FAIR) - Robotics<br>

	<!-- <br>
	Affiliate Associate Professor <br>
	<a href="https://www.cs.washington.edu/">Paul G. Allen School of Computer Science & Engineering</a> <br>
	<a href="https://www.washington.edu/">University of Washington</a> <br> -->

	<br>
	Email: roozbehm [at( gmail.com <div style="height:8px;font-size:1px;">&nbsp;</div>
	<a href="https://twitter.com/roozbehmottaghi?lang=en"> <img src="./images/x-logo.png" width="30" height="30"> </a> 
	<a href="https://www.semanticscholar.org/author/Roozbeh-Mottaghi/3012475"> <img src="./images/s2-logo.png" width="30" height="30"> </a>
	<a href="https://scholar.google.com/citations?user=CCV58dgAAAAJ&hl=en&oi=ao"> <img src="./images/scholar-logo.jpg" width="30" height="30"> </a>  

	</div>
</div>
</section>

<!-- Project Highlights -->
<section>
<h2 style="text-align: center">Project Highlights</h2>
<br>
<dl class="dl-horizontal">

  <dt><a href="https://arxiv.org/pdf/2504.00907">Ask-to-Act</a> (2025)</dt>
    <dd class="secondary">
      Using RL for training MLLMs in settings that an agent takes navigation actions and generates natural language questions.
    </dd>

  <dt><a href="https://ai.meta.com/blog/machine-intelligence-research-new-models/">PARTNR</a> (2025)</dt>
    <dd class="secondary">
      A benchmark and a suite of models for planning and reasoning in human-robot collaborative tasks. (<a href="https://www.facebook.com/share/p/1A9nJh3jy2/">Mark Zuckerberg announcement</a>, <a href="https://x.com/AIatMeta/status/1887970775090843804">AI at Meta channels</a>, <a href="https://youtu.be/JJX_U35xa7k?si=-T6L1OR_CYI_6nYE">YouTube</a>).
    </dd>

  <dt><a href="https://github.com/facebookresearch/habitat-sim">Habitat 3.0</a> (2024)</dt>
    <dd class="secondary">
      A simulator for humans and robots used to train and evaluate models performing collaborative tasks. (<a href="https://github.com/facebookresearch/habitat-sim">GitHub</a>, <a href="https://techcrunch.com/2023/10/20/embodied-ai-spins-a-pen-and-helps-clean-the-living-room-in-new-research/">TechCrunch</a>)
    </dd>

  <dt><a href="https://mattwallingford.github.io/ODIN/">ODIN</a> (2024)</dt>
    <dd class="secondary">
      A world model designed to imagine and reason about 3D environments, trained on our dataset of one million 360° videos.
    </dd>

  <dt><a href="https://homangab.github.io/track2act/">Track2Act</a> (2024)</dt>
    <dd class="secondary">
      Learning robot manipulation from large-scale internet videos.
    </dd>

  <dt><a href="https://theophilegervet.github.io/projects/goat/">GOAT</a>  (2024)</dt>
    <dd class="secondary">
      Go to AnyThing (GOAT) is a powerful robot navigation model that accepts targets defined through images, object categories, or natural-language descriptions.
    </dd>

  <dt><a href="https://unified-io.allenai.org/">Unified-IO</a> (2023)</dt>
    <dd class="secondary">
      A multi-modal model that unifies tasks with different types of inputs and outputs.
    </dd>

  <dt><a href="https://procthor.allenai.org/">ProcTHOR</a> (2022)</dt>
    <dd class="secondary">
      Large-scale Embodied AI using procedural generation. Won NeurIPS 2022 Outstanding Paper Award.
    </dd>

  <dt><a href="https://okvqa.allenai.org/">OK-VQA</a>  (2022)</dt> 
    <dd class="secondary">
      <a href="https://arxiv.org/pdf/1906.00067">OK-VQA</a> and <a href="https://arxiv.org/pdf/2206.01718">A-OKVQA</a> are popular benchmarks for for visual question answering using reasoning and world knowledge.
    </dd>

  <dt>Self-adaptation</dt>
    <dd class="secondary">
      <a href="https://arxiv.org/pdf/1812.00971">SAVN</a> (2018) and <a href="https://arxiv.org/pdf/1906.00067">Interactron </a> (2020) are self-adaptive models that continue training even during test-time interactions using meta-learning approaches. 
    </dd>

  <dt><a href="https://arxiv.org/pdf/1803.10827">DECADE</a> (2018)</dt>
    <dd class="secondary">
      Learning representations from a dataset we collected of dog interactions captured with cameras and motion sensors mounted on the dog body. (<a href="https://techcrunch.com/2018/04/11/whos-a-good-ai-dog-based-data-creates-a-canine-machine-learning-system/">TechCrunch</a>, <a href="https://www.technologyreview.com/s/610775/this-ai-thinks-like-a-dog/">MIT Technology Review</a>, <a href="https://www.nbcnews.com/mach/science/why-scientists-are-teaching-ai-think-dog-ncna869266">NBC News</a>)
    </dd>

  <dt><a href="https://ai2thor.allenai.org/">AI2-THOR</a> (2017)</dt>
    <dd class="secondary">
      A robot simulator developed for training and evaluating models designed for navigation, manipulation, and other robotics tasks. (<a href="https://github.com/allenai/ai2thor">GitHub</a>, <a href="https://spectrum.ieee.org/interactive-simulation-teaches-ai-about-real-world">IEEE Spectrum</a>, <a href="https://www.cbc.ca/news/science/ramona-pringle-robot-butler-fears-1.4567516">CBC News</a>)
    </dd>

  <dt><a href="https://arxiv.org/pdf/1609.05143">RL for Navigation</a> </dt>
    <dd class="secondary">
      Reinforcement Learning (RL) for training a robot navigation model. 
    </dd>
  
  <dt><a href="https://arxiv.org/pdf/1603.05600">ForScene</a> (2016)</dt>
    <dd class="secondary">
      Predicting the future movements of objects purely from visual observations. (<a href="https://www.technologyreview.com/2016/08/30/157826/what-robots-can-learn-from-babies/">MIT technology Review</a>)
    </dd>
  

</dl>


</section>


<!-- News -->
<section>
<h2 style="text-align: center">Highlights and News</h2>
<br>
<dl class="dl-horizontal">

  <dt>Nov, 2025:</dt>
  <dd>
    Giving invited talks at the <a href="https://holistic-video-understanding.github.io/">Holistic Video Understanding</a> and <a href="https://mozhgan91.github.io/vlm4rwd-neurips25-ws/"> Vision Language Models: Challenges of Real-World Deployment</a> workshops at NeurIPS 2025. I will talk about generating and understanding the 3D world and human-agent collaborative planning.
  </dd>

  <dt>Nov, 2025:</dt>
  <dd>
    Giving an invited talk at the  <a href="https://events.ucsc.edu/event/cse-colloquium-mitigating-data-scarcity-via-simulation-by-roozbeh-mottaghi/">CSE Colloquium at UCSC</a>. I will talk about mitigating data scarcity in robotics via simulation.
  </dd>

  <dt>Nov, 2025:</dt>
  <dd>
    Serving as the lead Area Chair for <a href="https://cvpr.thecvf.com/">CVPR 2026</a> and Area Chair for <a href="https://iclr.cc/">ICLR 2026</a>. 
  </dd>

  <dt>Oct, 2025:</dt>
  <dd>
    Giving invited talks at the <a href="https://heai-iros25-workshop.github.io/">Human-aware Embodied AI</a> and <a href="https://www.ai-meets-autonomy.com/">AI Meets Autonomy: Vision, Language, and Autonomous Systems</a> workshops at IROS 2025, and <a href="https://human-robot-scene.github.io/">Human-Robot-Scene Interaction and Collaboration</a> workshop at ICCV 2025.
  </dd>

  <dt>Jun, 2025:</dt>
  <dd>
    Giving an invited talk at the <a href="https://sites.google.com/view/gai-hri/home">Generative Modeling Meets Human-Robot Interaction</a> workshop at RSS 2025.
  </dd>

  <dt>Jun, 2025:</dt>
  <dd>
    Giving an invited talk at the <a href="https://3d-llm-vla.github.io/">3D-LLM/VLA: Bridging Language, Vision and Action in 3D Environments</a> and <a href="https://poets2024.github.io/poets2025/">Embodied Humans: Symbiotic Intelligence between Virtual Humans and Humanoid Robots</a> workshops at CVPR 2025.
  </dd>

  <dt>Apr, 2025:</dt>
  <dd>
    Giving an invited talk in <a href="https://ml.cs.sfu.ca/#/seminars">VCR/AI seminars</a> at Simon Fraser University. 
  </dd>
 
</dl>

<a href="news.html">Older items...</a>
</section>

<!-- About Me -->
<section>
<h2 style="text-align: center">About Me</h2>

<p>
I am a Senior AI Research Scientist Manager at FAIR and an Affiliate Associate Professor in <a href="https://www.cs.washington.edu/">Paul G. Allen School of Computer Science & Engineering</a> at the <a href="https://www.washington.edu/">University of Washington</a>. 
</p>	
<p>
Prior to joining FAIR, I was the Research Manager of the <a href="https://prior.allenai.org/">PRIOR</a> team at the <a href="https://allenai.org/">Allen Institute for AI</a>. Before that, I was a Postdoctoral Researcher in the <a href="http://www.cs.stanford.edu">Computer Science Department</a> at <a href="http://www.stanford.edu">Stanford University</a>. I received a Ph.D. in Computer Science from <a href="http://www.ucla.edu">UCLA</a>, advised by <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>. I obtained my Masters degrees from <a href="http://www.sfu.ca">Simon Fraser University</a> and <a href="http://www.gatech.edu">Georgia Institute of Technology</a> and my Bachelors degree from <a href="http://www.en.sharif.edu/">Sharif University of Technology</a>.
</p>

</section>

<section>
<a name="group"></a>
<h2 style="text-align: center">Students and Interns</h2>
<p>
I have had the pleasure of working with the following students, Pre-doctoral Young Investigators (PYI) aka residents, and interns.
</p>
<ul>
  <li>
    <a href="https://kianaehsani.com/">Kiana Ehsani</a>, PhD student (2016 &mdash; 2021); Co-advisor<br>Next position: Research Scientist at AI2
  </li>
  <li>
    <a href="https://homes.cs.washington.edu/~khzeng/">Kuo-hao Zeng</a>, PhD student (2018 &mdash; 2023); Co-advisor<br>Next position: Research Scientist at AI2
  </li>
  <li>
    <a href="https://mitchellnw.github.io/">Mitchell Wortsman</a>, AI2 PYI (2018 &mdash; 2019)<br>Next position: PhD student at the University of Washington
  </li>
  <li>
    <a href="https://klemenkotar.github.io/">Klemen Kotar</a>, AI2 PYI (2020 &mdash; 2022)
    <br>Next position: PhD student at Stanford University
  </li>  
  <li>
    <a href="https://apoorvkh.com/">Apoorv Khandelwal</a>, AI2 PYI (2020 &mdash; 2022)
    <br>Next position: PhD student at Brown University
  </li>
  <li>
    <a href="https://kunalmessi10.github.io/">Kunal Singh</a>, AI2 PYI (2021 &mdash; 2023)<br>Next position: PhD student at EPFL
  </li>
  <li>
    <a href="https://homangab.github.io/">Homanga Bharadhwaj</a>, Meta AIM student (2023 &mdash; 2024)<br>Next position: Research Scientist at Meta FAIR
  </li>
</ul>
  <h3>Interns</h3>
  <ul>
  <li class="packed"><a href="https://www.cs.utexas.edu/~yukez/">Yuke Zhu</a>, Stanford</li>  
  <li class="packed"><a href="https://danielgordon10.github.io/">Daniel Gordon</a>, University of Washington</li>  
  <li class="packed"><a href="http://kennethmarino.com/">Kenneth Marino</a>, CMU</li>  
  <li class="packed"><a href="https://arunmallya.github.io/">Arun Mallya</a>, UIUC</li>  
  <li class="packed"><a href="http://www.cs.cmu.edu/~fanyang1/">Fan Yang</a>, CMU</li>  
  <li class="packed"><a href="https://prithv1.xyz/">Prithvijit Chattopadhyay</a>, Georgia Tech</li>  
  <li class="packed"><a href="https://vishvakmurahari.com/">Vishvak Murahari</a>, Princeton</li>  
  <li class="packed"><a href="https://q-hwang.github.io/">Qian Huang</a>, Cornell</li>  
  <li class="packed"><a href="https://sagadre.github.io/">Samir Gadre</a>, Columbia University</li>  
  <li class="packed"><a href="https://kshitijd20.github.io/">Kshitij Dwivedi</a>, Goethe University</li>  
  <li class="packed"><a href="https://amandah3.web.illinois.edu/">Amanda Rose Yuile</a>, UIUC</li>  
  <li class="packed"><a href="https://scholar.google.com/citations?user=miFIAFMAAAAJ&hl=en">Peng Gao</a>, CUHK</li>  
  <li class="packed"><a href="https://jialinwu.netlify.app/">Jialin Wu</a>, UT Austin</li>  
  <li class="packed"><a href="https://sites.google.com/view/karlschmeckpeper">Karl Schmeckpeper</a>, U Penn</li>  
  <li class="packed"><a href="https://soyeonm.github.io/">So Yeon Tiffany Min</a>, CMU</li>  
  <li class="packed"><a href="https://ram81.github.io/">Ram Ramrakhya</a>, Georgia Tech</li>  
  
  

  </ul>

</section>

<section>
<A name=pubs></A> <h2 style="text-align: center">Publications</h2>
<ul>


  <li class="paper-with-image">
  <img src="images/colm25.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2504.04040">ADAPT: Actively Discovering and Adapting to Preferences for any Task</a>.<br>
    Maithili Patel, Xavier Puig, Ruta Desai, Roozbeh Mottaghi, Sonia Chernova, Joanne Truong, Akshara Rai
    <br>
    Conference on Language Modeling (CoLM), 2025. <br> 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/iclr25.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2411.00081">PARTNR: A Benchmark for Planning and Reasoning in Embodied Multi-agent Tasks</a>.<br>
    <i>(alphabetical order)</i> Matthew Chang, Gunjan Chhablani, Alexander Clegg, Mikael Dallaire Cote, Ruta Desai, Michal Hlavac, Vladimir Karashchuk, Jacob Krantz, Roozbeh Mottaghi, Priyam Parashar, Siddharth Patki, Ishita Prasad, Xavier Puig, Akshara Rai, Ram Ramrakhya, Daniel Tran, Joanne Truong, John M. Turner, Eric Undersander, Tsung-Yen Yang 
    <br>
    International Conference on Learning Representations (ICLR), 2025. <br> 
  </span>
  </li>


  <li class="paper-with-image">
  <img src="images/neurips24.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2412.07770v1">From an Image to a Scene: Learning to Imagine the World from a Million 360° Videos</a>.<br>
    Matthew Wallingford, Anand Bhattad, Aditya Kusupati, Vivek Ramanujan, Matt Deitke, Aniruddha Kembhavi, Roozbeh Mottaghi, Wei-Chiu Ma, Ali Farhadi
    <br>
    Advances in Neural Information Processing Systems (NeurIPS), 2024. <br> 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/eccv24_a.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2407.12061">Situated Instruction Following</a>.<br>
    So Yeon Min, Xavier Puig, Devendra Singh Chaplot, Tsung-Yen Yang, Priyam Parashar, Akshara Rai, Ruslan Salakhutdinov, Yonatan Bisk, Roozbeh Mottaghi
    <br>
    European Conference on Computer Vision (ECCV), 2024. <br> 
  </span>
  </li>


  <li class="paper-with-image">
  <img src="images/eccv24_b.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2405.01527">Track2Act: Predicting Point Tracks from Internet Videos enables Diverse Robot Manipulation</a>.<br>
    Homanga Bharadhwaj, Roozbeh Mottaghi*, Abhinav Gupta*, Shubham Tulsiani*
    <br>
    European Conference on Computer Vision (ECCV), 2024. <br> 
  </span>
  </li>

    <li class="paper-with-image">
  <img src="images/eccv24_c.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2312.03913">Controllable Human-Object Interaction Synthesis</a>.<br>
    Jiaman Li, Alexander Clegg, Roozbeh Mottaghi, Jiajun Wu, Xavier Puig*, C. Karen Liu*
    <br>
    European Conference on Computer Vision (ECCV), 2024. <br> 
    <b>Oral presentation</b>
  </span>
  </li>


  <li class="paper-with-image">
  <img src="images/rss24.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2311.06430">GOAT: GO to Any Thing</a>.<br>
    Matthew Chang, Theophile Gervet, Mukul Khanna, Sriram Yenamandra, Dhruv Shah, So Yeon Min, Kavit Shah, Chris Paxton, Saurabh Gupta, Dhruv Batra, Roozbeh Mottaghi, Jitendra Malik, Devendra Singh Chaplot
    <br>
    Robotics: Science and Systems (RSS), 2024. <br> 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/cvpr24.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2404.06609">GOAT-Bench: A Benchmark for Multi-Modal Lifelong Navigation</a>.<br>
    Mukul Khanna, Ram Ramrakhya, Gunjan Chhablani, Sriram Yenamandra, Theophile Gervet, Matthew Chang, Zsolt Kira, Devendra Singh Chaplot, Dhruv Batra, Roozbeh Mottaghi
    <br>
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. <br> 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/iclr24.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2310.13724.pdf">Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots</a>.<br>
    Xavier Puig*, Eric Undersander*, Andrew Szot*, Mikael Dallaire Cote*, Tsung-Yen Yang*, Ruslan Partsey*, Ruta Desai*, Alexander William Clegg*, Michal Hlavac, So Yeon Min, Vladimír Vondruš, Theophile Gervet, Vincent-Pierre Berges, John M. Turner, Oleksandr Maksymets, Zsolt Kira, Mrinal Kalakrishnan, Jitendra Malik, Devendra Singh Chaplot, Unnat Jain, Dhruv Batra, Akshara Rai†, Roozbeh Mottaghi† (*: core team, †: project leads)<br>
    International Conference on Learning Representations (ICLR), 2024. <br> 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/neurips23.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2306.10191.pdf">Neural Priming for Sample-Efficient Adaptation</a>.<br>
    Matthew Wallingford*, Vivek Ramanujan*, Alex Fang, Aditya Kusupati, Roozbeh Mottaghi, Aniruddha Kembhavi, Ludwig Schmidt, Ali Farhadi<br>
    Advances in Neural Information Processing Systems (NeurIPS), 2023. <br> 
  </span>
  </li>


  <li class="paper-with-image">
  <img src="images/corl23.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2306.11565.pdf">HomeRobot: Open-Vocabulary Mobile Manipulation</a>.<br>
    Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav, Austin Wang, Mukul Khanna, Theophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg, John Turner, Zsolt Kira, Manolis Savva, Angel Chang, Devendra Singh Chaplot, Dhruv Batra, Roozbeh Mottaghi, Yonatan Bisk, Chris Paxton<br>
    Conference on Robot Learning (CoRL), 2023. <br> 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/iccv23_a.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2304.02639.pdf">ENTL: Embodied Navigation Trajectory Learner</a>.<br>
    Klemen Kotar, Aaron Walsman, Roozbeh Mottaghi<br>
    International Conference on Computer Vision (ICCV), 2023. <br> 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/iccv23_b.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2304.01192.pdf">Navigating to Objects Specified by Images</a>.<br>
    Jacob Krantz, Theophile Gervet, Karmesh Yadav, Austin Wang, Chris Paxton, Roozbeh Mottaghi, Dhruv Batra, Jitendra Malik, Stefan Lee, Devendra Singh Chaplot<br>
    International Conference on Computer Vision (ICCV), 2023. <br> 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/cvpr23.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2306.07552.pdf">Galactic: Scaling End-to-End Reinforcement Learning for Rearrangement at 100k Steps-Per-Second</a>.<br>
    Vincent-Pierre Berges*, Andrew Szot*, Devendra Singh Chaplot, Aaron Gokaslan, Roozbeh Mottaghi, Dhruv Batra,  Eric Undersander (* equal contribution)<br>
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. <br> 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/iclr23_a.png" width=200 />
  <span>
    <a href="https://openreview.net/pdf?id=vmjctNUSWI">Moving Forward by Moving Backward: Embedding Action Impact over Action Semantics</a>.<br>
    Kuo-Hao Zeng, Luca Weihs, Roozbeh Mottaghi, Ali Farhadi<br>
    International Conference on Learning Representations (ICLR), 2023. <br> 
    <b>Oral presentation</b>
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/iclr23_b.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2206.08916.pdf">Unified-IO: A Unified Model for Vision, Language, and Multi-modal Tasks</a>.<br>
    Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, Aniruddha Kembhavi<br>
    International Conference on Learning Representations (ICLR), 2023. <br> 
    <b>Spotlight presentation</b>
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/iclr23_c.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2301.04101.pdf">Neural Radiance Field Codebooks</a>.<br>
    Matthew Wallingford, Aditya Kusupati, Alex Fang, Vivek Ramanujan, Aniruddha Kembhavi, Roozbeh Mottaghi, Ali Farhadi<br>
    International Conference on Learning Representations (ICLR), 2023. <br> 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/neurips22_a.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2206.06994.pdf">ProcTHOR: Large-Scale Embodied AI Using Procedural Generation</a>.<br>
    Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Jordi Salvador, Kiana Ehsani, Winson Han, Eric Kolve, Ali Farhadi, Aniruddha Kembhavi, Roozbeh Mottaghi<br>
    Advances in Neural Information Processing Systems (NeurIPS), 2022. <br> 
    <b><font color="FireBrick">Outstanding Paper Award</font> </b> <br>
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/neurips22_b.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2211.09960.pdf">Ask4Help: Learning to Leverage an Expert for Embodied Tasks</a>.<br>
    Kunal Pratap Singh, Luca Weihs, Alvaro Herrasti, Jonghyun Choi, Aniruddha Kembhavi, Roozbeh Mottaghi<br>
    Advances in Neural Information Processing Systems (NeurIPS), 2022. <br> 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/tmlr22.png" width=200 />
  <span>
    <a href="https://openreview.net/pdf?id=9NjqD9i48M">Benchmarking Progress to Infant-Level Physical Reasoning in AI</a>.<br>
    Luca Weihs, Amanda Yuile, Renée Baillargeon, Cynthia Fisher, Gary Marcus, Roozbeh Mottaghi, Aniruddha Kembhavi<br>
    Transactions on Machine Learning Research (TMLR), 2022. <br> 
  </span>
  </li>


  <li class="paper-with-image">
  <img src="images/eccv22_a.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2206.01718.pdf">A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge</a>.<br>
    Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, Roozbeh Mottaghi<br>
    European Conference on Computer Vision (ECCV), 2022. <br> 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/eccv22_b.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2203.08141.pdf">Object Manipulation via Visual Target Localization</a>.<br>
    Kiana Ehsani, Ali Farhadi, Aniruddha Kembhavi, Roozbeh Mottaghi<br>
    European Conference on Computer Vision (ECCV), 2022. <br> 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/cvpr22_a.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2202.00660.pdf">Interactron: Embodied Adaptive Object Detection</a>.<br>
    Klemen Kotar and Roozbeh Mottaghi<br>
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. <br> 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/cvpr22_b.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2203.17251.pdf">Continuous Scene Representations for Embodied AI</a>.<br>
    Samir Gadre, Kiana Ehsani, Shuran Song, Roozbeh Mottaghi<br>
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. <br> 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/cvpr22_c.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2206.08500.pdf">What do navigation agents learn about their environment?</a><br>
    Kshitij Dwivedi, Gemma Roig, Aniruddha Kembhavi, Roozbeh Mottaghi<br>
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. <br> 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/cvpr22_d.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2111.09888.pdf">Simple but Effective: CLIP Embeddings for Embodied AI</a>.<br>
    Apoorv Khandelwal*, Luca Weihs*, Roozbeh Mottaghi, Aniruddha Kembhavi (* equal contribution)<br>
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. <br> 
  </span>
  </li>


  <li class="paper-with-image">
  <img src="images/aaai22.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2103.12248.pdf">Multi-Modal Answer Validation for Knowledge-Based VQA</a>.<br>
    Jialin Wu, Jiasen Lu, Ashish Sabharwal, Roozbeh Mottaghi<br>
    AAAI Conference on Artificial Intelligence (AAAI), 2022. <br> 
    <b>Oral presentation</b> <br>
  </span>
  </li>


  <li class="paper-with-image">
  <img src="images/neurips21.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2106.01401.pdf">Container: Context Aggregation Networks</a>.<br>
    Gao Peng, Jiasen Lu, Hongsheng Li, Roozbeh Mottaghi, Aniruddha Kembhavi<br>
    Neural Information Processing Systems (NeurIPS), 2021. <br> 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/iccv21_a.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2106.04531.pdf">RobustNav : Towards Benchmarking Robustness in Embodied Navigation</a>.<br>
    Prithvijit Chattopadhyay, Judy Hoffman, Roozbeh Mottaghi, Aniruddha Kembhavi<br>
    International Conference on Computer Vision (ICCV), 2021. <br> 
    <b>Oral presentation</b> <br>
  </span>
  </li>


  <li class="paper-with-image">
  <img src="images/iccv21_b.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2103.14005.pdf">Contrasting Contrastive Self-Supervised Representation Learning Pipelines</a>.<br>
    Klemen Kotar, Gabriel Ilharco, Ludwig Schmidt, Kiana Ehsani, Roozbeh Mottaghi<br>
    International Conference on Computer Vision (ICCV), 2021. <br> 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/iccv21_c.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2012.03208.pdf">Factorizing Perception and Policy for Interactive Instruction Following</a>.<br>
    Kunal Singh, Suvaansh Bhambri, Byeonghwi Kim, Roozbeh Mottaghi, Jonghyun Choi<br>
    International Conference on Computer Vision (ICCV), 2021. <br> 
  </span>
  </li>


  <li class="paper-with-image">
  <img src="images/acl21.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2106.00188.pdf">PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World</a>.<br>
    Rowan Zellers, Ari Holtzman, Matthew Peters, Roozbeh Mottaghi, Aniruddha Kembhavi, Ali Farhadi,  Yejin Choi<br>
    The 59th Annual Meeting of the Association for Computational Linguistics (ACL), 2021. <br> 
    <b>Oral presentation</b> <br>
    [<a href="https://rowanzellers.com/piglet">Project page</a>]
  </span>
  </li>



  <li class="paper-with-image">
  <img src="images/cvpr21_a.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2103.16544.pdf">Visual Room Rearrangement</a>.<br>
    Luca Weihs, Matt Deitke, Aniruddha Kembhavi, Roozbeh Mottaghi<br>
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. <br> <b>Oral presentation</b> <br>
    [<a href="https://github.com/allenai/ai2thor-rearrangement">Room Rearrangement Code & Dataset</a>]
  </span>
  </li>


  <li class="paper-with-image">
  <img src="images/cvpr21_b.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2104.11213.pdf">ManipulaTHOR: A Framework for Visual Object Manipulation</a>.<br>
    Kiana Ehsani, Winson Han, Alvaro Herrasti, Eli VanderBilt, Eric Kolve, Luca Weihs, Aniruddha Kembhavi, Roozbeh Mottaghi <br>
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. <br> <b>Oral presentation</b> <br>
    [<a href="https://prior.allenai.org/projects/manipulathor">Project page</a>]
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/cvpr21_c.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2104.14040.pdf">Pushing it out of the Way: Interactive Visual Navigation</a>.<br>
    Kuo-hao Zeng, Luca Weihs, Ali Farhadi, Roozbeh Mottaghi<br>
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. <br> 
    [<a href="https://prior.allenai.org/projects/interactive-visual-navigation">Project page</a>]
  </span>
  </li>


  <li class="paper-with-image">
  <img src="images/iclr21_a.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/1912.08195.pdf">Learning Generalizable Visual Representations via Interactive Gameplay</a>.<br>
    Luca Weihs, Aniruddha Kembhavi, Kiana Ehsani, Sarah M. Pratt, Winson Han, Alvaro Herrasti, Eric Kolve, Dustin Schwenk, Roozbeh Mottaghi, Ali Farhadi<br>
    International Conference on Learning Representations (ICLR), 2021. <br><b>Oral presentation</b> <br>
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/iclr21_b.svg" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2010.08539.pdf">What Can You Learn from Your Muscles? Learning Visual Representation from Human Interactions</a>.<br>
    Kiana Ehsani, Daniel Gordon, Thomas Hai Dang Nguyen, Roozbeh Mottaghi, Ali Farhadi<br>
    International Conference on Learning Representations (ICLR), 2021. <br>
    [<a href="https://github.com/ehsanik/muscleTorch">Code & Dataset</a>]
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/rearrangement.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2011.01975.pdf">Rearrangement: A Challenge for Embodied AI</a>.<br>
    Dhruv Batra, Angel X. Chang, Sonia Chernova, Andrew J. Davison, Jia Deng, Vladlen Koltun, Sergey Levine, Jitendra Malik, Igor Mordatch, Roozbeh Mottaghi, Manolis Savva, Hao Su<br>
    arXiv, 2020. <br>
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/neurips20.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2006.09306.pdf">Learning About Objects by Learning to Interact with Them</a>.<br>
    Martin Lohmann, Jordi Salvador, Aniruddha Kembhavi, Roozbeh Mottaghi<br>
    Advances in Neural Information Processing Systems (NeurIPS), 2020. <br>
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/AllenAct.svg" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2008.12760.pdf">AllenAct: A Framework for Embodied AI Research</a>.<br>
    Luca Weihs*, Jordi Salvador*, Klemen Kotar*, Unnat Jain, Kuo-Hao Zeng, Roozbeh Mottaghi, Aniruddha Kembhavi. (* equal contribution)<br>
    arXiv, 2020. <br>
    [<a href="https://allenact.org/">AllenAct webpage</a>] 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/eccv20.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2004.10796.pdf">Visual Commonsense Graphs: Reasoning about the Dynamic Context of a Still Image</a>.<br>
    Jae Sung Park, Chandra Bhagavatula, Roozbeh Mottaghi, Ali Farhadi, Yejin Choi<br>
    European Conference on Computer Vision (ECCV), 2020. <br> <b>Spotlight presentation</b> <br>
    [<a href="https://visualcomet.xyz/">Project page</a>] 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/arxiv20.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2006.13171.pdf">ObjectNav Revisited: On Evaluation of Embodied Agents Navigating to Objects</a>.<br>
    Dhruv Batra, Aaron Gokaslan, Aniruddha Kembhavi, Oleksandr Maksymets, Roozbeh Mottaghi, Manolis Savva, Alexander Toshev, Erik Wijmans<br>
    arXiv, 2020. <br>
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/cvpr20_a.jpg" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2004.06799.pdf">RoboTHOR: An Open Simulation-to-Real Embodied AI Platform</a>.<br>

    Matt Deitke*, Winson Han*, Alvaro Herrasti*, Aniruddha Kembhavi*, Eric Kolve*,
    Roozbeh Mottaghi*, Jordi Salvador*, Dustin Schwenk*, Eli VanderBilt*, Matthew Wallingford*, Luca Weihs*, Mark Yatskar*, Ali Farhadi. (* alphabetically listed equal contribution)<br>
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. <br>  
    [<a href="https://ai2thor.allenai.org/robothor/">RoboTHOR webpage</a>] 
  </span>
  
  </li>

  <li class="paper-with-image">
  <img src="images/cvpr20_b.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/1912.02155.pdf">Visual Reaction: Learning To Play Catch With Your Drone</a>.<br>
    Kuo-Hao Zeng, Roozbeh Mottaghi, Luca Weihs, Ali Farhadi. <br>
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. <br>
    [<a href="https://prior.allenai.org/projects/visual-reaction">Project page</a>] 
  </span>
  
  </li>

    <li class="paper-with-image">
  <img src="images/cvpr20_c.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/1912.01734.pdf">ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks</a>.<br>
    Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox. <br>
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. <br>
    [<a href="https://askforalfred.com/">Project & Challenge page</a>] 
  </span>
  
  </li>

  <li class="paper-with-image">
  <img src="images/cvpr19_a.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/1812.00971.pdf">Learning to Learn How to Learn: Self-Adaptive Visual Navigation using Meta-Learning</a>.<br>
    Mitchell Wortsman, Kiana Ehsani, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi. <br>
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. <br><b>Oral presentation</b> <br>
    [<a href="https://prior.allenai.org/projects/savn">Project page</a>]    
  </span>
  
  </li>

  <li class="paper-with-image">
  <img src="images/cvpr19_b.png" width=200 />
  <span>
  <a href="https://arxiv.org/pdf/1906.00067.pdf">OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge</a>.<br>
  Kenneth Marino, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi.<br>
  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. <br>
  [<a href="https://okvqa.allenai.org/">Project page</a>]
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/iclr19.gif" width=200 />
  <span>
  <a href="https://arxiv.org/pdf/1810.06543.pdf">Visual Semantic Navigation using Scene Priors</a>.<br>
  Wei Yang, Xiaolong Wang, Ali Farhadi, Abhinav Gupta, Roozbeh Mottaghi.<br>
  International Conference on Learning Representations (ICLR), 2019.<br>
  [<a href="https://prior.allenai.org/projects/savn">Project page</a>]
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/navigation.png" width=200 />
  <span>
  <a href="https://arxiv.org/pdf/1807.06757.pdf">On Evaluation of Embodied Navigation Agents</a>.<br>
  Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, Amir R. Zamir.<br>
  arXiv, 2018.<br>
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/cvpr18_a.png" width=200 />
  <span>
  <a href="https://arxiv.org/pdf/1803.10827.pdf">Who Let The Dogs Out? Modeling Dog Behavior From Visual Data</a>.<br>
  Kiana Ehsani, Hessam Bagherinezhad, Joe Redmon, Roozbeh Mottaghi, Ali Farhadi.<br>
  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. <br>
  [<a href="https://github.com/ehsanik/dogTorch">Project page</a>]
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/cvpr18_b.png" width=200 />
  <span>    
  <a href="https://arxiv.org/pdf/1703.10239.pdf">SeGAN: Segmenting and Generating the Invisible</a>.<br>
  Kiana Ehsani, Roozbeh Mottaghi, Ali Farhadi.<br>
  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. <br><b>Spotlight presentation</b> <br>
  [<a href="https://github.com/ehsanik/SeGAN">Project page</a>]
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/ai2thor.png" width=200 />
  <span>
  <a href="https://arxiv.org/pdf/1712.05474.pdf">AI2-THOR: An Interactive 3D Environment for Visual AI</a>.<br>
  Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, Ali Farhadi.<br>
  arXiv, 2017.<br>
  [<a href="http://ai2thor.allenai.org/">http://ai2thor.allenai.org/</a>]
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/iccv17_a.png" width=200 />
  <span>
  <a href="https://arxiv.org/pdf/1705.08080.pdf">Visual Semantic Planning using Deep Successor Representations</a>.<br>
  Yuke Zhu*, Daniel Gordon*, Eric Kolve, Dieter Fox, Li Fei-Fei, Abhinav Gupta, Roozbeh Mottaghi, Ali Farhadi.<br>
  International Conference on Computer Vision (ICCV), 2017.<br>
  [<a href="https://prior.allenai.org/projects/visual-semantic-planning">Project page</a>]
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/iccv17_b.png" width=200 />
  <span>
  <a href="https://arxiv.org/pdf/1701.02718.pdf">See the Glass Half Full: Reasoning about Liquid Containers, their Volume and Content</a>.<br>
  Roozbeh Mottaghi, Connor Schenck, Dieter Fox, Ali Farhadi.<br>
  International Conference on Computer Vision (ICCV), 2017.<br>
  [<a href="https://prior.allenai.org/projects/see-the-glass-half-full">Project page</a>]
  </span>
  </li>  

  
  <li class="paper-with-image">
  <img src="images/icra17.png" width=200 />
  <span>
  <a href="https://arxiv.org/pdf/1609.05143.pdf">Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning</a>.<br>
  Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J. Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi.<br>
  International Conference on Robotics and Automation (ICRA), 2017.<br>
  [<a href="https://prior.allenai.org/projects/target-driven-visual-navigation">Project page</a>]
  </span>
  </li>  

  <li class="paper-with-image">
  <img src="images/eccv16_a.png" width=200 />
  <span>
  <a href="https://arxiv.org/pdf/1603.05600.pdf">"What happens if..." Learning to Predict the Effect of Forces in Images</a>.<br>
  Roozbeh Mottaghi, Mohammad Rastegari, Abhinav Gupta, Ali Farhadi.<br>
  European Conference on Computer Vision (ECCV), 2016.<br>
  [<a href="https://prior.allenai.org/projects/what-happens-if">Project page</a>]
  </span>
  </li>  

  <li class="paper-with-image">
  <img src="images/eccv16_b.png" width=200 />
  <span>
  <a href="papers/Xiang16eccv.pdf">ObjectNet3D: A Large Scale Database for 3D Object Recognition</a>.<br>
  Yu Xiang, Wonhui Kim, Wei Chen, Jingwei Ji, Christopher Choy, Hao Su, Roozbeh Mottaghi, Leonidas Guibas, Silvio  Savarese.<br>
  European Conference on Computer Vision (ECCV), 2016.  <br><b>Spotlight presentation</b><br>
  [<a href="http://cvgl.stanford.edu/projects/objectnet3d/">Project page</a>]
  </span>
  </li>  

  <li class="paper-with-image">
  <img src="images/cvpr16_a.png" width=200 />
  <span>
  <a href="papers/Mottaghi16cvpr_a.pdf">Newtonian Image Understanding: Unfolding the Dynamics of Objects in Static Images</a>.<br>
    Roozbeh Mottaghi, Hessam Bagherinezhad, Mohammad Rastegari, Ali Farhadi.<br>
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.  <br>
    [<a href="https://prior.allenai.org/projects/newtonian-image-understanding">Project page</a>]
  </li> 

  <li class="paper-with-image">
  <img src="images/cvpr16_b.png" width=200 />
  <span>
  <a href="papers/Mottaghi16cvpr_b.pdf">A Task-oriented Approach for Cost-sensitive Recognition</a>.<br>
    Roozbeh Mottaghi, Hannaneh Hajishirzi, Ali Farhadi.<br>
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.  <br>
    [<a href="https://prior.allenai.org/projects/task-oriented">Project page</a>][<a href="papers/Mottaghi16cvpr_b-sm.pdf">Supplementary Material</a>]
  </span>
  </li> 

  <li class="paper-with-image">
  <img src="images/pami15.png" width=200 />
  <span>
  <a href="papers/Mottaghi16pami.pdf">Human-Machine CRFs for Identifying Bottlenecks in Scene Understanding</a>.<br>
    Roozbeh Mottaghi, Sanja Fidler, Alan Yuille, Raquel Urtasun, Devi Parikh.<br>
    IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 38(1):74-87, 2016.<br>
    [<a href="papers/Mottaghi16pami-sm.pdf">Supplementary Material</a>]
  </span>    
  </li> 

  <li class="paper-with-image">
  <img src="images/jmlr15.png" width=200 />
  <span>
  <a href="http://jmlr.org/papers/v17/yuille16a.html">Complexity of Representation and Inference in Compositional Models with Part Sharing</a>.<br>
    Alan Yuille and Roozbeh Mottaghi.<br>
    Journal of Machine Learning Research (JMLR), 17(11):1-28, 2016.<br>
  </span>    
  </li> 

  <li class="paper-with-image">
  <img src="images/cvpr15.png" width=200 />
  <span>
  <a href="papers/Mottaghi15cvpr.pdf">A Coarse-to-Fine Model for 3D Pose Estimation and Sub-category Recognition</a>.<br>
    Roozbeh Mottaghi, Yu Xiang, and Silvio Savarese. <br>
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.<br>
    [<a href="papers/Mottaghi15cvpr-sm.pdf">Supplementary Material</a>][<A href="ftp://cs.stanford.edu/cs/cvgl/cvpr_15_annotation.tar.gz">dataset</A>][<A href="ftp://cs.stanford.edu/cs/cvgl/cad_models_cvpr15.tar.gz">CAD models</A>]
  </span>    
  </li> 

  <li class="paper-with-image">
  <img src="images/eccv14.png" width=200 />
  <span>
  <a href="papers/Xiang14eccv.pdf">Monocular Multiview Object Tracking with 3D Aspect Parts</a>.<br>
    Yu Xiang*, Changkyu Song*, Roozbeh Mottaghi and Silvio Savarese.<br>
    European Conference on Computer Vision (ECCV), 2014.<br>
  </span>    
  </li> 

  <li class="paper-with-image">
  <img src="images/wacv14.png" width=200 />
  <span>
  <a href="papers/Xiang14wacv.pdf">Beyond PASCAL: A Benchmark for 3D Object Detection in the Wild</a>.<br>
    Yu Xiang, Roozbeh Mottaghi, and Silvio Savarese.<br>
    IEEE Winter Conference on Applications of Computer Vision (WACV), 2014.<br>
    [<A href="http://cvgl.stanford.edu/projects/pascal3d.html">PASCAL 3D+ dataset</A>]
  </span>    
  </li> 

  <li class="paper-with-image">
  <img src="images/cvpr14_a.png" width=200 />
  <span>
  <a href="papers/Mottaghi14cvpr.pdf">The Role of Context for Object Detection and Semantic Segmentation in the Wild</a>.<br>
    Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, Alan Yuille.<br>
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.<br>
    [<A href="papers/errata.pdf">Errata</A>][<A href="pascal-context/">PASCAL Context dataset</A>]
  </span>    
  </li> 

  <li class="paper-with-image">
  <img src="images/cvpr14_b.png" width=200 />
  <span>
  <a href="papers/Chen14cvpr.pdf">Detect What You Can: Detecting and Representing Objects using Holistic Models and Body Parts</a>.<br>
    Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler, Raquel Urtasun, Alan Yuille.<br>
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.<br>
    [<A href="pascal-parts/pascal-parts.html">PASCAL Parts dataset</A>]
  </span>    
  </li> 

  <li class="paper-with-image">
  <img src="images/iclr13.png" width=200 />
  <span>
  <a href="http://arxiv.org/abs/1301.3560">Complexity of Representation and Inference in Compositional Models with Part Sharing</a>.<br> 
    Alan Yuille and Roozbeh Mottaghi.<br>
    International Conference on Learning Representations (ICLR), 2013. <br><b>Oral presentation</b><br>
  </span>    
  </li> 

  <li class="paper-with-image">
  <img src="images/cvpr13_a.png" width=200 />
  <span>
  <a href="papers/Mottaghi13cvpr_a.pdf">Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a>.<br> 
    Roozbeh Mottaghi, Sanja Fidler, Jian Yao, Raquel Urtasun, Devi Parikh.<br>
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013. <br>
    [<A href="papers/Mottaghi13cvpr_a-sm.pdf">Supplementary material</A>]
  </span>    
  </li> 

  <li class="paper-with-image">
  <img src="images/cvpr13_b.png" width=200 />
  <span>
  <a href="papers/Fidler13cvpr.pdf">Bottom-up Segmentation for Top-down Detection</a>.<br> 
    Sanja Fidler, Roozbeh Mottaghi, Alan Yuille, Raquel Urtasun.<br>
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013. <br>
    [<A href="https://www.cs.utoronto.ca/~fidler/projects/segDPM.html">Project page</A>]
  </span>    
  </li> 

  <li class="paper-with-image">
  <img src="images/cvpr12.png" width=200 />
  <span>
  <a href="papers/Mottaghi12cvpr.pdf">Augmenting Deformable Part Models with Irregular-shaped Object Patches</a>.<br> 
    Roozbeh Mottaghi.<br>
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012. <br>
    [<A href="papers/Mottaghi12cvpr-sm.pdf">Supplementary material</A>]
  </span>    
  </li> 

  <li class="paper-with-image">
  <img src="images/iccvw11.png" width=200 />
  <span>
  <a href="papers/Mottaghi11iccvw.pdf">A Compositional Approach to Learning Part-based Models of Objects</a>.<br> 
    Roozbeh Mottaghi, Ananth Ranganathan, and Alan Yuille.<br>
    Workshop on 3D Representation and Recognition, held with the International Conference on Computer Vision (ICCV), 2011. <br>
    [<A href="misc/HOGBundle.tar.gz">Code</A>]
  </span>    
  </li>   

  <li class="paper-with-image">
  <img src="images/icra09.jpg" width=200 />
  <span>
  <a href="papers/Lee09icra.pdf">Graph-based Planning Using Local Information for Unknown Outdoor Environments</a>.<br> 
    Jinhan Lee, Roozbeh Mottaghi, Charles Pippin, and Tucker Balch.<br>
    International Conference on Robotics and Automation (ICRA), 2009. <br>
  </span>    
  </li>   

  <li class="paper-with-image">
  <img src="images/icra08.jpg" width=200 />
  <span>
  <a href="papers/Mottaghi08icra.pdf">Place Recognition-based Fixed-lag Smoothing for Environments with Unreliable GPS</a>.<br> 
    Roozbeh Mottaghi, Michael Kaess, Ananth Ranganathan, Richard Roberts, and Frank Dellaert.<br>
    International Conference on Robotics and Automation (ICRA), 2008. <br>
    [<A href="movies/placerec.mp4">Video</A>]
  </span>    
  </li>   

  <li class="paper-with-image">
  <img src="images/autorob.jpg" width=200 />
  <span>
  <a href="papers/Mottaghi07autorob.pdf">An Integrated Particle Filter and Potential Field Method Applied to Multi-Robot Target Tracking</a>.<br> 
    Roozbeh Mottaghi and Richard Vaughan.<br>
    Autonomous Robots, 23(1):19-35, 2007. <br>
    [Videos: <A href="movies/tworec.mp4">clip 1</A> and <A href="movies/4ro-3.avi">clip 2</A>]
  </span>    
  </li>   

  <li class="paper-with-image">
  <img src="images/icra06.jpg" width=200 />
  <span>
  <a href="papers/Mottaghi06icra.pdf">An Integrated Particle Filter & Potential Field Method for Cooperative Robot Target Tracking</a>.<br> 
    Roozbeh Mottaghi and Richard Vaughan.<br>
    International Conference on Robotics and Automation (ICRA), 2006. <br>    
  </span>    
  </li>   

  <li class="paper-with-image">
  <img src="images/icar05.jpg" width=200 />
  <span>
  <a href="papers/Mottaghi05icar.pdf">An Overview of a Probabilistic Tracker for Multiple Cooperative Tracking Agents</a>.<br> 
    Roozbeh Mottaghi and Shahram Payandeh.<br>
    International Conference on Advanced Robotics (ICAR), 2005. <br>    
  </span>    
  </li>   

  <li class="paper-with-image">
  <img src="images/crv05.jpg" width=200 />
  <span>
  <a href="papers/Mottaghi05crv.pdf">Coordination of Multiple Agents for Probabilistic Object Tracking</a>.<br> 
    Roozbeh Mottaghi and Shahram Payandeh.<br>
    Canadian Conference on Computer and Robot Vision (CRV), 2005. <br>    
  </span>    
  </li>   

  <li class="paper-with-image">
  <img src="images/robocup.jpg" width=200 />
  <span>
  <a href="papers/Manzuri02LNAI.pdf">SharifCESR Small Size Robocup Team</a>.<br> 
    Mohammad T. Manzuri, Hamid R. Chitsaz, Reza Ghorbani, Pooya Karimian, Alireza Mirazi, Mehran Motamed, Roozbeh Mottaghi and Payam Sabzmeydani.<br>
    Robocup 2001: Robot Soccer World Cup V. Lecture Notes in Artificial Intelligence 2377, 2002. <br>    
  </span>    
  </li>   
</ul>

</section>


<!-- Press -->
<section>
<h2 style="text-align: center">Press Coverage</h2>

<ul>

  <li>
  <i>Habitat 3.0, HomeRobot, and Habitat Synthetic Scenes Dataset</i>.<br>
  
  <table style="text-align: center; width: 100px;" border="0" cellpadding="0" cellspacing="3">
  <tbody>
  <tr>

  <td>
    <a href="https://techcrunch.com/2023/10/20/embodied-ai-spins-a-pen-and-helps-clean-the-living-room-in-new-research/">
    <img style="border:2px solid gray; padding:2px; width: 208px; height: 30px;" src="./images/techcrunch-logo.png">
    </a>
  </td>

  </tr>
  </tbody>
  </table>
  </li>

  <li>
  <i>AllenAct Framework</i>.<br>
  
  <table style="text-align: center; width: 100px;" border="0" cellpadding="0" cellspacing="3">
  <tbody>
  <tr>

  <td>
    <a href="https://venturebeat.com/2020/08/31/allen-institute-open-sources-allenact-a-framework-for-research-in-embodied-ai/">
    <img style="border:2px solid gray; padding:2px; width: 137px; height: 30px;" src="./images/VentureBeat-logo.png">
    </a>
  </td>

  </tr>
  </tbody>
  </table>
  </li>

  <li>
  <i>RoboTHOR Challenge</i>.<br>
  
  <table style="text-align: center; width: 100px;" border="0" cellpadding="0" cellspacing="3">
  <tbody>
  <tr>

  <td>
    <a href="https://www.geekwire.com/2020/ai2-throws-challenge-robotic-scavenger-hunt-set-virtual-real-rooms/">
    <img style="border:2px solid gray; padding:2px; width: 137px; height: 30px;" src="./images/GeekWire-logo.png">
    </a>
  </td>

  <td>
    <a href="https://www.technologyreview.com/s/615186/ai-ai2-robots-navigate-world-train-algorithms-challenge/">
    <img style="border:2px solid gray; padding:2px; width: 70px; height: 30px;" src="./images/mit-tech-review-logo.png">
    </a>
  </td>


  </tr>
  </tbody>
  </table>
  </li>

  <li>
  <i>AI2-THOR Project</i>.<br>
  
  <table style="text-align: center; width: 100px;" border="0" cellpadding="0" cellspacing="3">
  <tbody>
  <tr>

  <td>
    <a href="https://spectrum.ieee.org/interactive-simulation-teaches-ai-about-real-world">
    <img style="border:2px solid gray; padding:2px; width: 150px; height: 30px;" src="./images/ieee-spectrum-logo.jpg">
    </a>
  </td>

  <td>
    <a href="https://www.digitaltrends.com/cool-tech/virtual-reality-training-for-robots/">
    <img style="border:2px solid gray; padding:2px; width: 197px; height: 30px;" src="./images/DT-logo.png">
    </a>
  </td>

  <td>
    <a href="https://www.cbc.ca/news/technology/ramona-pringle-robot-butler-fears-1.4567516">
    <img style="border:2px solid gray; padding:2px; width: 59px; height: 30px;" src="./images/cbc-logo.jpg">
    </a>
  </td>

  </tr>
  </tbody>
  </table>
  </li>

  <li>
  <i>Dog Project</i>.
  
  <table style="text-align: center; width: 100px;" border="0" cellpadding="0" cellspacing="3">
  <tbody>
  <tr>

  <td>
    <a href="https://techcrunch.com/2018/04/11/whos-a-good-ai-dog-based-data-creates-a-canine-machine-learning-system/">
    <img style="border:2px solid gray; padding:2px; width: 208px; height: 30px;" src="./images/techcrunch-logo.png">
    </a>
  </td>

  <td>
    <a href="https://www.technologyreview.com/s/610775/this-ai-thinks-like-a-dog/">
    <img style="border:2px solid gray; padding:2px; width: 70px; height: 30px;" src="./images/mit-tech-review-logo.png">
    </a>
  </td>

  <td>
    <a href="http://www.pbs.org/wgbh/nova/next/tech/ai-trained-to-act-like-a-dog/">
    <img style="border:2px solid gray; padding:2px; width: 75px; height: 30px;" src="./images/PBS_logo.png">
    </a>
  </td>

  <td>
    <a href="https://www.theverge.com/2018/4/14/17234570/artificial-intelligence-dogs-research-science-learning">
    <img style="border:2px solid gray; padding:2px; width: 58px; height: 30px;" src="./images/verge-logo.png">
    </a>
  </td>

  <td>
    <a href="https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/real-dog-behavior-could-inspire-robot-dogs">
    <img style="border:2px solid gray; padding:2px; width: 150px; height: 30px;" src="./images/ieee-spectrum-logo.jpg">
    </a>
  </td>  

  <td>
    <a href="https://www.nbcnews.com/mach/science/why-scientists-are-teaching-ai-think-dog-ncna869266">
    <img style="border:2px solid gray; padding:2px; width: 37px; height: 30px;" src="./images/nbc-logo.png">
    </a>
  </td>    

  <td>
    <a href="http://www.euronews.com/2018/04/26/why-scientists-are-teaching-ai-think-dog-ncna869266">
    <img style="border:2px solid gray; padding:2px; width: 114px; height: 30px;" src="./images/euronews-logo.png">
    </a>
  </td>    

  <td>
    <a href="https://www.reuters.com/video/2018/06/14/dog-vision-project-analyses-canine-behav?videoId=435834651&videoChannel=118065&channelName=Moments+of+Innovation">
    <img style="border:2px solid gray; padding:2px; width: 29px; height: 30px;" src="./images/Reuters-Logo.jpg">
    </a>
  </td> 

  </tr>
  </tbody>
  </table>
  </li>


  <li>
  <i>Prediction Project</i>.<br>
  
  <table style="text-align: center; width: 100px;" border="0" cellpadding="0" cellspacing="3">
  <tbody>
  <tr>

  <td>
    <a href="https://www.technologyreview.com/s/602246/what-robots-can-learn-from-babies/">
    <img style="border:2px solid gray; padding:2px; width: 70px; height: 30px;" src="./images/mit-tech-review-logo.png">
    </a>
  </td>

  </tr>
  </tbody>
  </table>
  </li>

  <li>
  <i>AI documentary (Can we build a brain?)</i><br>
  
  <table style="text-align: center; width: 100px;" border="0" cellpadding="0" cellspacing="3">
  <tbody>
  <tr>

  <td>
    <a href="https://www.pbs.org/video/nova-wonders-can-we-build-a-brain-j53aqg/">
    <img style="border:2px solid gray; padding:2px; width: 75px; height: 30px;" src="./images/PBS_logo.png">
    </a>
  </td>

  </tr>
  </tbody>
  </table>
  </li>


</ul>

<br>
</section>



<!-- Footer -->
<footer>
<h4>© Roozbeh Mottaghi</h4>
<h6>(Adopted the template from <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>'s website)</h6>
</footer>

<!-- Start of StatCounter Code -->
<script type="text/javascript" language="javascript">
var sc_project=1498370; 
var sc_invisible=1; 
var sc_partition=13; 
var sc_security="250b5be7"; 
</script>

<script type="text/javascript" language="javascript" src="https://www.statcounter.com/counter/counter.js"></script><noscript><a href="https://www.statcounter.com/" target="_blank"><img  src="https://c14.statcounter.com/counter.php?sc_project=1498370&amp;java=0&amp;security=250b5be7&amp;invisible=1" alt="free stats" border="0"></a> </noscript>
<!-- End of StatCounter Code -->


</body></html>
