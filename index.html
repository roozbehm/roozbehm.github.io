<!DOCTYPE html>
<!-- saved from url=(0034)https://www.cc.gatech.edu/~dbatra/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
 
 <meta name="description" content="Research Manager (AI2) and Affiliate Associate Professor (University of Washington)">
 <meta name="og:description" content="Research Manager (AI2) and Affiliate Associate Professor (University of Washington)">
 <meta name="robots" content="all">
 <title>Roozbeh Mottaghi's Webpage</title>
 <link rel="stylesheet" href="./misc/mystyle.css">
 <link rel="stylesheet" href="./misc/pygments.css">
</head>

<body>

<!-- Header -->
<header>
<h2><a href="index.html">Roozbeh Mottaghi</a></h2>
<nav>
<ul>
 <li><a href="index.html">Home</a></li>
  <li><a href="#pubs">Publications</a></li>
  <li><a href="misc/CV.pdf">CV</a></li>
</ul>
</nav>
</header>

<!-- Affiliations + Contact -->
<section style="height: 250px;">
<div class ="profile">
	<img src="./images/profile_roozbehM.jpg" align="left" width="230" style="margin-top:5px; margin-right:81px" alt="me">
	<div>
	<b>Roozbeh Mottaghi</b>
	<br><br>
	Research Scientist Manager <br>
	FAIR<br>
	<a href="https://ai.facebook.com/">Meta AI</a> <br>

	<br>
	Affiliate Associate Professor <br>
	<a href="https://www.cs.washington.edu/">Paul G. Allen School of Computer Science & Engineering</a> <br>
	<a href="https://www.washington.edu/">University of Washington</a> <br>

	<br>
	Email: roozbehm [at( fb.com <div style="height:8px;font-size:1px;">&nbsp;</div>
	<a href="https://twitter.com/roozbehmottaghi?lang=en"> <img src="./images/twitter-logo.png" width="30" height="30"> </a> 
	<a href="https://www.semanticscholar.org/author/Roozbeh-Mottaghi/3012475"> <img src="./images/s2-logo.png" width="30" height="30"> </a>
	<a href="https://scholar.google.com/citations?user=CCV58dgAAAAJ&hl=en&oi=ao"> <img src="./images/scholar-logo.jpg" width="30" height="30"> </a>  

	</div>
</div>
</section>


<!-- News -->
<section>
<h2 style="text-align: center">Highlights and News</h2>
<br>
<dl class="dl-horizontal">

  <dt>Aug, 2022:</dt>
  <dd>
    Joined FAIR at Meta AI as a Research Manager to lead part of the Embodied AI efforts at FAIR.
  </dd>
	

  <dt>Jun, 2022:</dt>
  <dd>
    Giving an invited talk at the <a href="https://embodied-ai.org/">Embodied AI Workshop</a> at CVPR 2022.
  </dd>

  <dt>Apr, 2022:</dt>
  <dd>
    Giving an invited talk at Stanford HAI Metaverse Workshop.
  </dd>

  <dt>Mar, 2022:</dt>
  <dd>
    Giving an invited talk at <a href="http://www.csig.org.cn/detail/3329">China Society of Image and Graphics (CSIG)</a>.
  </dd>

  <dt>Oct, 2021:</dt>
  <dd>
    Giving an invited talk at the ICCV 2021 workshop on <a href="https://geometry.stanford.edu/struco3d/">Structural and Compositional Learning on 3D Data</a>.
  </dd>

  <dt>Jun, 2021:</dt>
  <dd>
    Giving invited talks at the following CVPR 2021 workshops: <a href="https://sites.google.com/view/cvpr2021-3d-vision-robotics/">3D Vision and Robotics</a>, <a href="https://scene-understanding.com/">3D Scene Understanding for Vision, Graphics, and Robotics</a>, and <a href="https://learn3dg.github.io/">Learning to Generate 3D Shapes and Scenes</a>.
  </dd>

  <dt>May, 2021:</dt>
  <dd>
    Giving a guest lecture at Stanford <a href="https://web.stanford.edu/class/cs331b/">CS331B: Interactive Simulation for Robot Learning</a>.
  </dd>

    

</dl>

<a href="news.html">Older items...</a>
</section>

<!-- About Me -->
<section>
<h2 style="text-align: center">About Me</h2>

<p>
I am a Research Scientist Manager at FAIR and an Affiliate Associate Professor in <a href="https://www.cs.washington.edu/">Paul G. Allen School of Computer Science & Engineering</a> at the <a href="https://www.washington.edu/">University of Washington</a>. 
</p>	
<p>
Prior to joining FAIR, I was the Research Manager of the <a href="https://prior.allenai.org/">PRIOR</a> team at the <a href="https://allenai.org/">Allen Institute for AI</a>. Before that, I was a Postdoctoral Researcher in the <a href="http://www.cs.stanford.edu">Computer Science Department</a> at <a href="http://www.stanford.edu">Stanford University</a>. I received a Ph.D. in Computer Science from <a href="http://www.ucla.edu">UCLA</a>, advised by <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>. I obtained my Masters degrees from <a href="http://www.sfu.ca">Simon Fraser University</a> and <a href="http://www.gatech.edu">Georgia Institute of Technology</a> and my Bachelors degree from <a href="http://www.en.sharif.edu/">Sharif University of Technology</a>.
</p>

</section>

<section>
<a name="group"></a>
<h2 style="text-align: center">Students and Interns</h2>
<p>
I have had the pleasure of working with the following students, Pre-doctoral Young Investigators (PYI) aka residents, and interns.
</p>
<ul>
  <li>
    <a href="https://sites.google.com/view/ehsanik-personal-website">Kiana Ehsani</a>, PhD student (2016 &mdash; 2021); Co-advisor<br>Next position: Research Scientist at AI2
  </li>
  <li>
    <a href="https://homes.cs.washington.edu/~khzeng/">Kuo-hao Zeng</a>, PhD student (2018 &mdash; Present); Co-advisor
  </li>
  <li>
    <a href="https://mitchellnw.github.io/">Mitchell Wortsman</a>, PYI (2018 &mdash; 2019)<br>Next position: PhD student at the University of Washington
  </li>
  <li>
    <a href="https://klemenkotar.github.io/">Klemen Kotar</a>, PYI (2020 &mdash; 2022)
    <br>Next position: PhD student at Stanford University
  </li>  
  <li>
    <a href="https://apoorvkh.com/">Apoorv Khandelwal</a>, PYI (2020 &mdash; 2022)
    <br>Next position: PhD student at Brown University
  </li>
  <li>
    <a href="https://kunalmessi10.github.io/">Kunal Singh</a>, PYI (2021 &mdash; Present)
  </li>
</ul>
  <h3>Interns</h3>
  <ul>
  <li class="packed"><a href="https://www.cs.utexas.edu/~yukez/">Yuke Zhu</a>, Stanford</li>  
  <li class="packed"><a href="https://danielgordon10.github.io/">Daniel Gordon</a>, University of Washington</li>  
  <li class="packed"><a href="http://kennethmarino.com/">Kenneth Marino</a>, CMU</li>  
  <li class="packed"><a href="https://arunmallya.github.io/">Arun Mallya</a>, UIUC</li>  
  <li class="packed"><a href="http://www.cs.cmu.edu/~fanyang1/">Fan Yang</a>, CMU</li>  
  <li class="packed"><a href="https://prithv1.xyz/">Prithvijit Chattopadhyay</a>, Georgia Tech</li>  
  <li class="packed"><a href="https://vishvakmurahari.com/">Vishvak Murahari</a>, Princeton</li>  
  <li class="packed"><a href="https://q-hwang.github.io/">Qian Huang</a>, Cornell</li>  
  <li class="packed"><a href="https://sagadre.github.io/">Samir Gadre</a>, Columbia University</li>  
  <li class="packed"><a href="https://kshitijd20.github.io/">Kshitij Dwivedi</a>, Goethe University</li>  
  <li class="packed"><a href="https://amandah3.web.illinois.edu/">Amanda Rose Yuile</a>, UIUC</li>  
  <li class="packed"><a href="https://scholar.google.com/citations?user=miFIAFMAAAAJ&hl=en">Peng Gao</a>, CUHK</li>  
  <li class="packed"><a href="https://jialinwu.netlify.app/">Jialin Wu</a>, UT Austin</li>  
  </ul>

</section>

<section>
<A name=pubs></A> <h2 style="text-align: center">Publications</h2>
<ul>


  <li class="paper-with-image">
  <img src="images/eccv22_a.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2206.01718.pdf">A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge</a>.<br>
    Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, Roozbeh Mottaghi<br>
    European Conference on Computer Vision (ECCV), 2022. <br> 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/eccv22_b.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2203.08141.pdf">Object Manipulation via Visual Target Localization</a>.<br>
    Kiana Ehsani, Ali Farhadi, Aniruddha Kembhavi, Roozbeh Mottaghi<br>
    European Conference on Computer Vision (ECCV), 2022. <br> 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/cvpr22_a.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2202.00660.pdf">Interactron: Embodied Adaptive Object Detection</a>.<br>
    Klemen Kotar and Roozbeh Mottaghi<br>
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. <br> 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/cvpr22_b.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2203.17251.pdf">Continuous Scene Representations for Embodied AI</a>.<br>
    Samir Gadre, Kiana Ehsani, Shuran Song, Roozbeh Mottaghi<br>
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. <br> 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/cvpr22_c.png" width=200 />
  <span>
    <a href="#">What do navigation agents learn about their environment?</a><br>
    Kshitij Dwivedi, Gemma Roig, Aniruddha Kembhavi, Roozbeh Mottaghi<br>
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. <br> 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/cvpr22_d.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2111.09888.pdf">Simple but Effective: CLIP Embeddings for Embodied AI</a>.<br>
    Apoorv Khandelwal*, Luca Weihs*, Roozbeh Mottaghi, Aniruddha Kembhavi (* equal contribution)<br>
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. <br> 
  </span>
  </li>


  <li class="paper-with-image">
  <img src="images/aaai22.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2103.12248.pdf">Multi-Modal Answer Validation for Knowledge-Based VQA</a>.<br>
    Jialin Wu, Jiasen Lu, Ashish Sabharwal, Roozbeh Mottaghi<br>
    AAAI Conference on Artificial Intelligence (AAAI), 2022. <br> 
    <b>Oral presentation</b> <br>
  </span>
  </li>


  <li class="paper-with-image">
  <img src="images/neurips21.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2106.01401.pdf">Container: Context Aggregation Networks</a>.<br>
    Gao Peng, Jiasen Lu, Hongsheng Li, Roozbeh Mottaghi, Aniruddha Kembhavi<br>
    Neural Information Processing Systems (NeurIPS), 2021. <br> 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/iccv21_a.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2106.04531.pdf">RobustNav : Towards Benchmarking Robustness in Embodied Navigation</a>.<br>
    Prithvijit Chattopadhyay, Judy Hoffman, Roozbeh Mottaghi, Aniruddha Kembhavi<br>
    International Conference on Computer Vision (ICCV), 2021. <br> 
    <b>Oral presentation</b> <br>
  </span>
  </li>


  <li class="paper-with-image">
  <img src="images/iccv21_b.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2103.14005.pdf">Contrasting Contrastive Self-Supervised Representation Learning Pipelines</a>.<br>
    Klemen Kotar, Gabriel Ilharco, Ludwig Schmidt, Kiana Ehsani, Roozbeh Mottaghi<br>
    International Conference on Computer Vision (ICCV), 2021. <br> 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/iccv21_c.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2012.03208.pdf">Factorizing Perception and Policy for Interactive Instruction Following</a>.<br>
    Kunal Singh, Suvaansh Bhambri, Byeonghwi Kim, Roozbeh Mottaghi, Jonghyun Choi<br>
    International Conference on Computer Vision (ICCV), 2021. <br> 
  </span>
  </li>


  <li class="paper-with-image">
  <img src="images/acl21.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2106.00188.pdf">PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World</a>.<br>
    Rowan Zellers, Ari Holtzman, Matthew Peters, Roozbeh Mottaghi, Aniruddha Kembhavi, Ali Farhadi,  Yejin Choi<br>
    The 59th Annual Meeting of the Association for Computational Linguistics (ACL), 2021. <br> 
    <b>Oral presentation</b> <br>
    [<a href="https://rowanzellers.com/piglet">Project page</a>]
  </span>
  </li>



  <li class="paper-with-image">
  <img src="images/cvpr21_a.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2103.16544.pdf">Visual Room Rearrangement</a>.<br>
    Luca Weihs, Matt Deitke, Aniruddha Kembhavi, Roozbeh Mottaghi<br>
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. <br> <b>Oral presentation</b> <br>
    [<a href="https://github.com/allenai/ai2thor-rearrangement">Room Rearrangement Code & Dataset</a>]
  </span>
  </li>


  <li class="paper-with-image">
  <img src="images/cvpr21_b.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2104.11213.pdf">ManipulaTHOR: A Framework for Visual Object Manipulation</a>.<br>
    Kiana Ehsani, Winson Han, Alvaro Herrasti, Eli VanderBilt, Eric Kolve, Luca Weihs, Aniruddha Kembhavi, Roozbeh Mottaghi <br>
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. <br> <b>Oral presentation</b> <br>
    [<a href="https://prior.allenai.org/projects/manipulathor">Project page</a>]
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/cvpr21_c.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2104.14040.pdf">Pushing it out of the Way: Interactive Visual Navigation</a>.<br>
    Kuo-hao Zeng, Luca Weihs, Ali Farhadi, Roozbeh Mottaghi<br>
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. <br> 
    [<a href="https://prior.allenai.org/projects/interactive-visual-navigation">Project page</a>]
  </span>
  </li>


  <li class="paper-with-image">
  <img src="images/iclr21_a.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/1912.08195.pdf">Learning Generalizable Visual Representations via Interactive Gameplay</a>.<br>
    Luca Weihs, Aniruddha Kembhavi, Kiana Ehsani, Sarah M. Pratt, Winson Han, Alvaro Herrasti, Eric Kolve, Dustin Schwenk, Roozbeh Mottaghi, Ali Farhadi<br>
    International Conference on Learning Representations (ICLR), 2021. <br><b>Oral presentation</b> <br>
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/iclr21_b.svg" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2010.08539.pdf">What Can You Learn from Your Muscles? Learning Visual Representation from Human Interactions</a>.<br>
    Kiana Ehsani, Daniel Gordon, Thomas Hai Dang Nguyen, Roozbeh Mottaghi, Ali Farhadi<br>
    International Conference on Learning Representations (ICLR), 2021. <br>
    [<a href="https://github.com/ehsanik/muscleTorch">Code & Dataset</a>]
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/rearrangement.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2011.01975.pdf">Rearrangement: A Challenge for Embodied AI</a>.<br>
    Dhruv Batra, Angel X. Chang, Sonia Chernova, Andrew J. Davison, Jia Deng, Vladlen Koltun, Sergey Levine, Jitendra Malik, Igor Mordatch, Roozbeh Mottaghi, Manolis Savva, Hao Su<br>
    arXiv, 2020. <br>
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/neurips20.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2006.09306.pdf">Learning About Objects by Learning to Interact with Them</a>.<br>
    Martin Lohmann, Jordi Salvador, Aniruddha Kembhavi, Roozbeh Mottaghi<br>
    Neural Information Processing Systems (NeurIPS), 2020. <br>
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/AllenAct.svg" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2008.12760.pdf">AllenAct: A Framework for Embodied AI Research</a>.<br>
    Luca Weihs*, Jordi Salvador*, Klemen Kotar*, Unnat Jain, Kuo-Hao Zeng, Roozbeh Mottaghi, Aniruddha Kembhavi. (* equal contribution)<br>
    arXiv, 2020. <br>
    [<a href="https://allenact.org/">AllenAct webpage</a>] 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/eccv20.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2004.10796.pdf">Visual Commonsense Graphs: Reasoning about the Dynamic Context of a Still Image</a>.<br>
    Jae Sung Park, Chandra Bhagavatula, Roozbeh Mottaghi, Ali Farhadi, Yejin Choi<br>
    European Conference on Computer Vision (ECCV), 2020. <br> <b>Spotlight presentation</b> <br>
    [<a href="https://visualcomet.xyz/">Project page</a>] 
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/arxiv20.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2006.13171.pdf">ObjectNav Revisited: On Evaluation of Embodied Agents Navigating to Objects</a>.<br>
    Dhruv Batra, Aaron Gokaslan, Aniruddha Kembhavi, Oleksandr Maksymets, Roozbeh Mottaghi, Manolis Savva, Alexander Toshev, Erik Wijmans<br>
    arXiv, 2020. <br>
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/cvpr20_a.jpg" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/2004.06799.pdf">RoboTHOR: An Open Simulation-to-Real Embodied AI Platform</a>.<br>

    Matt Deitke*, Winson Han*, Alvaro Herrasti*, Aniruddha Kembhavi*, Eric Kolve*,
    Roozbeh Mottaghi*, Jordi Salvador*, Dustin Schwenk*, Eli VanderBilt*, Matthew Wallingford*, Luca Weihs*, Mark Yatskar*, Ali Farhadi. (* alphabetically listed equal contribution)<br>
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. <br>  
    [<a href="https://ai2thor.allenai.org/robothor/">RoboTHOR webpage</a>] 
  </span>
  
  </li>

  <li class="paper-with-image">
  <img src="images/cvpr20_b.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/1912.02155.pdf">Visual Reaction: Learning To Play Catch With Your Drone</a>.<br>
    Kuo-Hao Zeng, Roozbeh Mottaghi, Luca Weihs, Ali Farhadi. <br>
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. <br>
    [<a href="https://prior.allenai.org/projects/visual-reaction">Project page</a>] 
  </span>
  
  </li>

    <li class="paper-with-image">
  <img src="images/cvpr20_c.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/1912.01734.pdf">ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks</a>.<br>
    Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox. <br>
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. <br>
    [<a href="https://askforalfred.com/">Project & Challenge page</a>] 
  </span>
  
  </li>

  <li class="paper-with-image">
  <img src="images/cvpr19_a.png" width=200 />
  <span>
    <a href="https://arxiv.org/pdf/1812.00971.pdf">Learning to Learn How to Learn: Self-Adaptive Visual Navigation using Meta-Learning</a>.<br>
    Mitchell Wortsman, Kiana Ehsani, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi. <br>
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. <br><b>Oral presentation</b> <br>
    [<a href="https://prior.allenai.org/projects/savn">Project page</a>]    
  </span>
  
  </li>

  <li class="paper-with-image">
  <img src="images/cvpr19_b.png" width=200 />
  <span>
  <a href="https://arxiv.org/pdf/1906.00067.pdf">OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge</a>.<br>
  Kenneth Marino, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi.<br>
  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. <br>
  [<a href="https://okvqa.allenai.org/">Project page</a>]
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/iclr19.gif" width=200 />
  <span>
  <a href="https://arxiv.org/pdf/1810.06543.pdf">Visual Semantic Navigation using Scene Priors</a>.<br>
  Wei Yang, Xiaolong Wang, Ali Farhadi, Abhinav Gupta, Roozbeh Mottaghi.<br>
  International Conference on Learning Representations (ICLR), 2019.<br>
  [<a href="https://prior.allenai.org/projects/savn">Project page</a>]
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/navigation.png" width=200 />
  <span>
  <a href="https://arxiv.org/pdf/1807.06757.pdf">On Evaluation of Embodied Navigation Agents</a>.<br>
  Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, Amir R. Zamir.<br>
  arXiv, 2018.<br>
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/cvpr18_a.png" width=200 />
  <span>
  <a href="https://arxiv.org/pdf/1803.10827.pdf">Who Let The Dogs Out? Modeling Dog Behavior From Visual Data</a>.<br>
  Kiana Ehsani, Hessam Bagherinezhad, Joe Redmon, Roozbeh Mottaghi, Ali Farhadi.<br>
  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. <br>
  [<a href="https://github.com/ehsanik/dogTorch">Project page</a>]
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/cvpr18_b.png" width=200 />
  <span>    
  <a href="https://arxiv.org/pdf/1703.10239.pdf">SeGAN: Segmenting and Generating the Invisible</a>.<br>
  Kiana Ehsani, Roozbeh Mottaghi, Ali Farhadi.<br>
  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. <br><b>Spotlight presentation</b> <br>
  [<a href="https://github.com/ehsanik/SeGAN">Project page</a>]
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/ai2thor.png" width=200 />
  <span>
  <a href="https://arxiv.org/pdf/1712.05474.pdf">AI2-THOR: An Interactive 3D Environment for Visual AI</a>.<br>
  Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, Ali Farhadi.<br>
  arXiv, 2017.<br>
  [<a href="http://ai2thor.allenai.org/">http://ai2thor.allenai.org/</a>]
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/iccv17_a.png" width=200 />
  <span>
  <a href="https://arxiv.org/pdf/1705.08080.pdf">Visual Semantic Planning using Deep Successor Representations</a>.<br>
  Yuke Zhu*, Daniel Gordon*, Eric Kolve, Dieter Fox, Li Fei-Fei, Abhinav Gupta, Roozbeh Mottaghi, Ali Farhadi.<br>
  International Conference on Computer Vision (ICCV), 2017.<br>
  [<a href="https://prior.allenai.org/projects/visual-semantic-planning">Project page</a>]
  </span>
  </li>

  <li class="paper-with-image">
  <img src="images/iccv17_b.png" width=200 />
  <span>
  <a href="https://arxiv.org/pdf/1701.02718.pdf">See the Glass Half Full: Reasoning about Liquid Containers, their Volume and Content</a>.<br>
  Roozbeh Mottaghi, Connor Schenck, Dieter Fox, Ali Farhadi.<br>
  International Conference on Computer Vision (ICCV), 2017.<br>
  [<a href="https://prior.allenai.org/projects/see-the-glass-half-full">Project page</a>]
  </span>
  </li>  

  
  <li class="paper-with-image">
  <img src="images/icra17.png" width=200 />
  <span>
  <a href="https://arxiv.org/pdf/1609.05143.pdf">Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning</a>.<br>
  Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J. Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi.<br>
  International Conference on Robotics and Automation (ICRA), 2017.<br>
  [<a href="https://prior.allenai.org/projects/target-driven-visual-navigation">Project page</a>]
  </span>
  </li>  

  <li class="paper-with-image">
  <img src="images/eccv16_a.png" width=200 />
  <span>
  <a href="https://arxiv.org/pdf/1603.05600.pdf">"What happens if..." Learning to Predict the Effect of Forces in Images</a>.<br>
  Roozbeh Mottaghi, Mohammad Rastegari, Abhinav Gupta, Ali Farhadi.<br>
  European Conference on Computer Vision (ECCV), 2016.<br>
  [<a href="https://prior.allenai.org/projects/what-happens-if">Project page</a>]
  </span>
  </li>  

  <li class="paper-with-image">
  <img src="images/eccv16_b.png" width=200 />
  <span>
  <a href="papers/Xiang16eccv.pdf">ObjectNet3D: A Large Scale Database for 3D Object Recognition</a>.<br>
  Yu Xiang, Wonhui Kim, Wei Chen, Jingwei Ji, Christopher Choy, Hao Su, Roozbeh Mottaghi, Leonidas Guibas, Silvio  Savarese.<br>
  European Conference on Computer Vision (ECCV), 2016.  <br><b>Spotlight presentation</b><br>
  [<a href="http://cvgl.stanford.edu/projects/objectnet3d/">Project page</a>]
  </span>
  </li>  

  <li class="paper-with-image">
  <img src="images/cvpr16_a.png" width=200 />
  <span>
  <a href="papers/Mottaghi16cvpr_a.pdf">Newtonian Image Understanding: Unfolding the Dynamics of Objects in Static Images</a>.<br>
    Roozbeh Mottaghi, Hessam Bagherinezhad, Mohammad Rastegari, Ali Farhadi.<br>
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.  <br>
    [<a href="https://prior.allenai.org/projects/newtonian-image-understanding">Project page</a>]
  </li> 

  <li class="paper-with-image">
  <img src="images/cvpr16_b.png" width=200 />
  <span>
  <a href="papers/Mottaghi16cvpr_b.pdf">A Task-oriented Approach for Cost-sensitive Recognition</a>.<br>
    Roozbeh Mottaghi, Hannaneh Hajishirzi, Ali Farhadi.<br>
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.  <br>
    [<a href="https://prior.allenai.org/projects/task-oriented">Project page</a>][<a href="papers/Mottaghi16cvpr_b-sm.pdf">Supplementary Material</a>]
  </span>
  </li> 

  <li class="paper-with-image">
  <img src="images/pami15.png" width=200 />
  <span>
  <a href="papers/Mottaghi16pami.pdf">Human-Machine CRFs for Identifying Bottlenecks in Scene Understanding</a>.<br>
    Roozbeh Mottaghi, Sanja Fidler, Alan Yuille, Raquel Urtasun, Devi Parikh.<br>
    IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 38(1):74-87, 2016.<br>
    [<a href="papers/Mottaghi16pami-sm.pdf">Supplementary Material</a>]
  </span>    
  </li> 

  <li class="paper-with-image">
  <img src="images/jmlr15.png" width=200 />
  <span>
  <a href="http://jmlr.org/papers/v17/yuille16a.html">Complexity of Representation and Inference in Compositional Models with Part Sharing</a>.<br>
    Alan Yuille and Roozbeh Mottaghi.<br>
    Journal of Machine Learning Research (JMLR), 17(11):1-28, 2016.<br>
  </span>    
  </li> 

  <li class="paper-with-image">
  <img src="images/cvpr15.png" width=200 />
  <span>
  <a href="papers/Mottaghi15cvpr.pdf">A Coarse-to-Fine Model for 3D Pose Estimation and Sub-category Recognition</a>.<br>
    Roozbeh Mottaghi, Yu Xiang, and Silvio Savarese. <br>
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.<br>
    [<a href="papers/Mottaghi15cvpr-sm.pdf">Supplementary Material</a>][<A href="ftp://cs.stanford.edu/cs/cvgl/cvpr_15_annotation.tar.gz">dataset</A>][<A href="ftp://cs.stanford.edu/cs/cvgl/cad_models_cvpr15.tar.gz">CAD models</A>]
  </span>    
  </li> 

  <li class="paper-with-image">
  <img src="images/eccv14.png" width=200 />
  <span>
  <a href="papers/Xiang14eccv.pdf">Monocular Multiview Object Tracking with 3D Aspect Parts</a>.<br>
    Yu Xiang*, Changkyu Song*, Roozbeh Mottaghi and Silvio Savarese.<br>
    European Conference on Computer Vision (ECCV), 2014.<br>
  </span>    
  </li> 

  <li class="paper-with-image">
  <img src="images/wacv14.png" width=200 />
  <span>
  <a href="papers/Xiang14wacv.pdf">Beyond PASCAL: A Benchmark for 3D Object Detection in the Wild</a>.<br>
    Yu Xiang, Roozbeh Mottaghi, and Silvio Savarese.<br>
    IEEE Winter Conference on Applications of Computer Vision (WACV), 2014.<br>
    [<A href="http://cvgl.stanford.edu/projects/pascal3d.html">PASCAL 3D+ dataset</A>]
  </span>    
  </li> 

  <li class="paper-with-image">
  <img src="images/cvpr14_a.png" width=200 />
  <span>
  <a href="papers/Mottaghi14cvpr.pdf">The Role of Context for Object Detection and Semantic Segmentation in the Wild</a>.<br>
    Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, Alan Yuille.<br>
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.<br>
    [<A href="papers/errata.pdf">Errata</A>][<A href="pascal-context/">PASCAL Context dataset</A>]
  </span>    
  </li> 

  <li class="paper-with-image">
  <img src="images/cvpr14_b.png" width=200 />
  <span>
  <a href="papers/Chen14cvpr.pdf">Detect What You Can: Detecting and Representing Objects using Holistic Models and Body Parts</a>.<br>
    Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler, Raquel Urtasun, Alan Yuille.<br>
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.<br>
    [<A href="pascal-parts/pascal-parts.html">PASCAL Parts dataset</A>]
  </span>    
  </li> 

  <li class="paper-with-image">
  <img src="images/iclr13.png" width=200 />
  <span>
  <a href="http://arxiv.org/abs/1301.3560">Complexity of Representation and Inference in Compositional Models with Part Sharing</a>.<br> 
    Alan Yuille and Roozbeh Mottaghi.<br>
    International Conference on Learning Representations (ICLR), 2013. <br><b>Oral presentation</b><br>
  </span>    
  </li> 

  <li class="paper-with-image">
  <img src="images/cvpr13_a.png" width=200 />
  <span>
  <a href="papers/Mottaghi13cvpr_a.pdf">Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a>.<br> 
    Roozbeh Mottaghi, Sanja Fidler, Jian Yao, Raquel Urtasun, Devi Parikh.<br>
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013. <br>
    [<A href="papers/Mottaghi13cvpr_a-sm.pdf">Supplementary material</A>]
  </span>    
  </li> 

  <li class="paper-with-image">
  <img src="images/cvpr13_b.png" width=200 />
  <span>
  <a href="papers/Fidler13cvpr.pdf">Bottom-up Segmentation for Top-down Detection</a>.<br> 
    Sanja Fidler, Roozbeh Mottaghi, Alan Yuille, Raquel Urtasun.<br>
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013. <br>
    [<A href="https://www.cs.utoronto.ca/~fidler/projects/segDPM.html">Project page</A>]
  </span>    
  </li> 

  <li class="paper-with-image">
  <img src="images/cvpr12.png" width=200 />
  <span>
  <a href="papers/Mottaghi12cvpr.pdf">Augmenting Deformable Part Models with Irregular-shaped Object Patches</a>.<br> 
    Roozbeh Mottaghi.<br>
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012. <br>
    [<A href="papers/Mottaghi12cvpr-sm.pdf">Supplementary material</A>]
  </span>    
  </li> 

  <li class="paper-with-image">
  <img src="images/iccvw11.png" width=200 />
  <span>
  <a href="papers/Mottaghi11iccvw.pdf">A Compositional Approach to Learning Part-based Models of Objects</a>.<br> 
    Roozbeh Mottaghi, Ananth Ranganathan, and Alan Yuille.<br>
    Workshop on 3D Representation and Recognition, held with the International Conference on Computer Vision (ICCV), 2011. <br>
    [<A href="misc/HOGBundle.tar.gz">Code</A>]
  </span>    
  </li>   

  <li class="paper-with-image">
  <img src="images/icra09.jpg" width=200 />
  <span>
  <a href="papers/Lee09icra.pdf">Graph-based Planning Using Local Information for Unknown Outdoor Environments</a>.<br> 
    Jinhan Lee, Roozbeh Mottaghi, Charles Pippin, and Tucker Balch.<br>
    International Conference on Robotics and Automation (ICRA), 2009. <br>
  </span>    
  </li>   

  <li class="paper-with-image">
  <img src="images/icra08.jpg" width=200 />
  <span>
  <a href="papers/Mottaghi08icra.pdf">Place Recognition-based Fixed-lag Smoothing for Environments with Unreliable GPS</a>.<br> 
    Roozbeh Mottaghi, Michael Kaess, Ananth Ranganathan, Richard Roberts, and Frank Dellaert.<br>
    International Conference on Robotics and Automation (ICRA), 2008. <br>
    [<A href="movies/placerec.mp4">Video</A>]
  </span>    
  </li>   

  <li class="paper-with-image">
  <img src="images/autorob.jpg" width=200 />
  <span>
  <a href="papers/Mottaghi07autorob.pdf">An Integrated Particle Filter and Potential Field Method Applied to Multi-Robot Target Tracking</a>.<br> 
    Roozbeh Mottaghi and Richard Vaughan.<br>
    Autonomous Robots, 23(1):19-35, 2007. <br>
    [Videos: <A href="movies/tworec.mp4">clip 1</A> and <A href="movies/4ro-3.avi">clip 2</A>]
  </span>    
  </li>   

  <li class="paper-with-image">
  <img src="images/icra06.jpg" width=200 />
  <span>
  <a href="papers/Mottaghi06icra.pdf">An Integrated Particle Filter & Potential Field Method for Cooperative Robot Target Tracking</a>.<br> 
    Roozbeh Mottaghi and Richard Vaughan.<br>
    International Conference on Robotics and Automation (ICRA), 2006. <br>    
  </span>    
  </li>   

  <li class="paper-with-image">
  <img src="images/icar05.jpg" width=200 />
  <span>
  <a href="papers/Mottaghi05icar.pdf">An Overview of a Probabilistic Tracker for Multiple Cooperative Tracking Agents</a>.<br> 
    Roozbeh Mottaghi and Shahram Payandeh.<br>
    International Conference on Advanced Robotics (ICAR), 2005. <br>    
  </span>    
  </li>   

  <li class="paper-with-image">
  <img src="images/crv05.jpg" width=200 />
  <span>
  <a href="papers/Mottaghi05crv.pdf">Coordination of Multiple Agents for Probabilistic Object Tracking</a>.<br> 
    Roozbeh Mottaghi and Shahram Payandeh.<br>
    Canadian Conference on Computer and Robot Vision (CRV), 2005. <br>    
  </span>    
  </li>   

  <li class="paper-with-image">
  <img src="images/robocup.jpg" width=200 />
  <span>
  <a href="papers/Manzuri02LNAI.pdf">SharifCESR Small Size Robocup Team</a>.<br> 
    Mohammad T. Manzuri, Hamid R. Chitsaz, Reza Ghorbani, Pooya Karimian, Alireza Mirazi, Mehran Motamed, Roozbeh Mottaghi and Payam Sabzmeydani.<br>
    Robocup 2001: Robot Soccer World Cup V. Lecture Notes in Artificial Intelligence 2377, 2002. <br>    
  </span>    
  </li>   
</ul>

</section>


<!-- Press -->
<section>
<h2 style="text-align: center">Press Coverage</h2>

<ul>

  <li>
  <i>AllenAct Framework</i>.<br>
  
  <table style="text-align: center; width: 100px;" border="0" cellpadding="0" cellspacing="3">
  <tbody>
  <tr>

  <td>
    <a href="https://venturebeat.com/2020/08/31/allen-institute-open-sources-allenact-a-framework-for-research-in-embodied-ai/">
    <img style="border:2px solid gray; padding:2px; width: 137px; height: 30px;" src="./images/VentureBeat-logo.png">
    </a>
  </td>

  </tr>
  </tbody>
  </table>
  </li>

  <li>
  <i>RoboTHOR Challenge</i>.<br>
  
  <table style="text-align: center; width: 100px;" border="0" cellpadding="0" cellspacing="3">
  <tbody>
  <tr>

  <td>
    <a href="https://www.geekwire.com/2020/ai2-throws-challenge-robotic-scavenger-hunt-set-virtual-real-rooms/">
    <img style="border:2px solid gray; padding:2px; width: 137px; height: 30px;" src="./images/GeekWire-logo.png">
    </a>
  </td>

  <td>
    <a href="https://www.technologyreview.com/s/615186/ai-ai2-robots-navigate-world-train-algorithms-challenge/">
    <img style="border:2px solid gray; padding:2px; width: 70px; height: 30px;" src="./images/mit-tech-review-logo.png">
    </a>
  </td>


  </tr>
  </tbody>
  </table>
  </li>

  <li>
  <i>AI2-THOR Project</i>.<br>
  
  <table style="text-align: center; width: 100px;" border="0" cellpadding="0" cellspacing="3">
  <tbody>
  <tr>

  <td>
    <a href="https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/interactive-simulation-teaches-ai-about-real-world">
    <img style="border:2px solid gray; padding:2px; width: 150px; height: 30px;" src="./images/ieee-spectrum-logo.jpg">
    </a>
  </td>

  <td>
    <a href="https://www.digitaltrends.com/cool-tech/virtual-reality-training-for-robots/">
    <img style="border:2px solid gray; padding:2px; width: 197px; height: 30px;" src="./images/DT-logo.png">
    </a>
  </td>

  <td>
    <a href="https://www.cbc.ca/news/technology/ramona-pringle-robot-butler-fears-1.4567516">
    <img style="border:2px solid gray; padding:2px; width: 59px; height: 30px;" src="./images/cbc-logo.jpg">
    </a>
  </td>

  </tr>
  </tbody>
  </table>
  </li>

  <li>
  <i>Dog Project</i>.
  
  <table style="text-align: center; width: 100px;" border="0" cellpadding="0" cellspacing="3">
  <tbody>
  <tr>

  <td>
    <a href="https://techcrunch.com/2018/04/11/whos-a-good-ai-dog-based-data-creates-a-canine-machine-learning-system/">
    <img style="border:2px solid gray; padding:2px; width: 208px; height: 30px;" src="./images/techcrunch-logo.png">
    </a>
  </td>

  <td>
    <a href="https://www.technologyreview.com/s/610775/this-ai-thinks-like-a-dog/">
    <img style="border:2px solid gray; padding:2px; width: 70px; height: 30px;" src="./images/mit-tech-review-logo.png">
    </a>
  </td>

  <td>
    <a href="http://www.pbs.org/wgbh/nova/next/tech/ai-trained-to-act-like-a-dog/">
    <img style="border:2px solid gray; padding:2px; width: 75px; height: 30px;" src="./images/PBS_logo.png">
    </a>
  </td>

  <td>
    <a href="https://www.theverge.com/2018/4/14/17234570/artificial-intelligence-dogs-research-science-learning">
    <img style="border:2px solid gray; padding:2px; width: 58px; height: 30px;" src="./images/verge-logo.png">
    </a>
  </td>

  <td>
    <a href="https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/real-dog-behavior-could-inspire-robot-dogs">
    <img style="border:2px solid gray; padding:2px; width: 150px; height: 30px;" src="./images/ieee-spectrum-logo.jpg">
    </a>
  </td>  

  <td>
    <a href="https://www.nbcnews.com/mach/science/why-scientists-are-teaching-ai-think-dog-ncna869266">
    <img style="border:2px solid gray; padding:2px; width: 37px; height: 30px;" src="./images/nbc-logo.png">
    </a>
  </td>    

  <td>
    <a href="http://www.euronews.com/2018/04/26/why-scientists-are-teaching-ai-think-dog-ncna869266">
    <img style="border:2px solid gray; padding:2px; width: 114px; height: 30px;" src="./images/euronews-logo.png">
    </a>
  </td>    

  <td>
    <a href="https://www.reuters.com/video/2018/06/14/dog-vision-project-analyses-canine-behav?videoId=435834651&videoChannel=118065&channelName=Moments+of+Innovation">
    <img style="border:2px solid gray; padding:2px; width: 29px; height: 30px;" src="./images/Reuters-Logo.jpg">
    </a>
  </td> 

  </tr>
  </tbody>
  </table>
  </li>


  <li>
  <i>Prediction Project</i>.<br>
  
  <table style="text-align: center; width: 100px;" border="0" cellpadding="0" cellspacing="3">
  <tbody>
  <tr>

  <td>
    <a href="https://www.technologyreview.com/s/602246/what-robots-can-learn-from-babies/">
    <img style="border:2px solid gray; padding:2px; width: 70px; height: 30px;" src="./images/mit-tech-review-logo.png">
    </a>
  </td>

  </tr>
  </tbody>
  </table>
  </li>

  <li>
  <i>AI documentary (Can we build a brain?)</i><br>
  
  <table style="text-align: center; width: 100px;" border="0" cellpadding="0" cellspacing="3">
  <tbody>
  <tr>

  <td>
    <a href="https://www.pbs.org/video/nova-wonders-can-we-build-a-brain-j53aqg/">
    <img style="border:2px solid gray; padding:2px; width: 75px; height: 30px;" src="./images/PBS_logo.png">
    </a>
  </td>

  </tr>
  </tbody>
  </table>
  </li>


</ul>

<br>
</section>



<!-- Footer -->
<footer>
<h4>© Roozbeh Mottaghi</h4>
<h6>(Adopted the template from <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>'s website)</h6>
</footer>

<!-- Start of StatCounter Code -->
<script type="text/javascript" language="javascript">
var sc_project=1498370; 
var sc_invisible=1; 
var sc_partition=13; 
var sc_security="250b5be7"; 
</script>

<script type="text/javascript" language="javascript" src="https://www.statcounter.com/counter/counter.js"></script><noscript><a href="https://www.statcounter.com/" target="_blank"><img  src="https://c14.statcounter.com/counter.php?sc_project=1498370&amp;java=0&amp;security=250b5be7&amp;invisible=1" alt="free stats" border="0"></a> </noscript>
<!-- End of StatCounter Code -->


</body></html>
