<!DOCTYPE html>
<!-- saved from url=(0034)https://www.cc.gatech.edu/~dbatra/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
 
 <meta name="description" content="Senior Research Scientist (AI2) and Affiliate Associate Professor (University of Washington)">
 <meta name="og:description" content="Senior Research Scientist (AI2) and Affiliate Associate Professor (University of Washington)">
 <title>Roozbeh Mottaghi</title>
 <link rel="stylesheet" href="./misc/mystyle.css">
 <link rel="stylesheet" href="./misc/pygments.css">
</head>

<body>

<!-- Header -->
<header>
<h2><a href="./index.html">Roozbeh Mottaghi</a></h2>
<nav>
<ul>
 <li><a href="./index.html">Home</a></li>
  <li><a href="./index.html#pubs">Publications</a></li>
  <li><a href="./code.html">Code</a></li>
  <li><a href="./misc/CV.pdf">CV</a></li>
</ul>
</nav>
</header>

<!-- News -->
<section>
<h2 style="text-align: center">Highlights and News</h2>

<dl class="dl-horizontal">

  <dt>Apr, 2022:</dt>
  <dd>
    Giving an invited talk at Stanford HAI Metaverse Workshop.
  </dd>

  <dt>Mar, 2022:</dt>
  <dd>
    Giving an invited talk at the Chinese Society of Image and Graphics (CSIG).
  </dd>

  <dt>Oct, 2021:</dt>
  <dd>
    Giving an invited talk at the ICCV 2021 workshop on <a href="https://geometry.stanford.edu/struco3d/">Structural and Compositional Learning on 3D Data</a>.
  </dd>

  <dt>Jun, 2021:</dt>
  <dd>
    Giving invited talks at the following CVPR 2021 workshops: <a href="https://sites.google.com/view/cvpr2021-3d-vision-robotics/">3D Vision and Robotics</a>, <a href="https://scene-understanding.com/">3D Scene Understanding for Vision, Graphics, and Robotics</a>, and <a href="https://learn3dg.github.io/">Learning to Generate 3D Shapes and Scenes</a>.
  </dd> 

  <dt>May, 2021:</dt>
  <dd>
    Giving a guest lecture at Stanford <a href="https://web.stanford.edu/class/cs331b/">CS331B: Interactive Simulation for Robot Learning</a>.
  </dd>

  <dt>Feb, 2021:</dt>
  <dd>
    We are organizing three challenges at the <a href="https://embodied-ai.org/">Embodied AI Workshop</a> at CVPR 2021: <a href="https://ai2thor.allenai.org/robothor/cvpr-2021-challenge/">Navigation towards objects</a>, <a href="https://ai2thor.allenai.org/rearrangement/">Room rearrangement</a>, and <a href="https://askforalfred.com/EAI21/">Interactive instruction following</a>.
  </dd>
  
  <dt>Nov, 2020:</dt>
  <dd>
    We released a report describing a new challenge for Embodied AI. This is a joint work with colleagues from Georgia Tech, FAIR, Simon Fraser University, Imperial College London, Princeton, Intel Labs, UC Berkeley, Google and UC San Diego. The report can be <a href="https://arxiv.org/pdf/2011.01975.pdf">accessed here</a>.
  </dd>

  <dt>Nov, 2020:</dt>
  <dd>
    Serving as Area Chair for <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>.
  </dd>
  
  <dt>Sep, 2020:</dt>
  <dd>
    We released <a href="https://allenact.org/">AllenAct</a>, a framework for unifying environments, models and training algorithms used in Embodied AI. Check out the details in this <a href="https://arxiv.org/pdf/2008.12760.pdf">arXiv paper</a>.
  </dd>
  
  <dt>Feb, 2020:</dt>
  <dd>
    Co-organizing <a href="https://askforalfred.com/EVAL/">Embodied Vision, Actions & Language Workshop</a> at <a href="https://eccv2020.eu/">ECCV 2020</a>, which hosts a challenge on <a href="https://askforalfred.com/">ALFRED</a> our embodied instruction following framework. 
  </dd>

  <dt>Feb, 2020:</dt>
  <dd>
    Co-organizing <a href="https://embodied-ai.org/">Embodied AI Workshop</a> at <a href="http://cvpr2020.thecvf.com/">CVPR 2020</a>. 
  </dd>

  <dt>Feb, 2020:</dt>
  <dd>
    Announcing the RoboTHOR navigation challenge. Follow <a href="https://ai2thor.allenai.org/robothor/challenge/">this link</a> for further information.
  </dd>
    
  <dt>Nov, 2019:</dt>
  <dd>
    Serving as Area Chair for <a href="http://cvpr2020.thecvf.com/">CVPR 2020</a>.
  </dd>

  <dt>Jun, 2019:</dt>
  <dd>
    Recognized as a <a href="http://cvpr2019.thecvf.com/">CVPR 2019</a> outstanding reviewer.
  </dd>

  <dt>Feb, 2019:</dt>
  <dd>
    Our papers on self-adaptive navigation and knowledge-based question answering have been accepted to <a href="http://cvpr2019.thecvf.com/">CVPR 2019</a>.
  </dd>

  <dt>Jan, 2019:</dt>
  <dd>  Our paper on navigation using scene knowledge was accepted to <a href="https://iclr.cc/">ICLR 2019</a>.</dd>

  <dt>Jul, 2018:</dt>
  <dd>We published a <a href="https://arxiv.org/pdf/1807.06757.pdf">paper</a> about evaluation of navigation agents. </dd>

  <dt>May, 2018:</dt>
  <dd>Our work was covered in a documentary on AI by PBS. Here is the link to the video: <a href="https://www.pbs.org/video/nova-wonders-can-we-build-a-brain-j53aqg/">Can we build a brain?</a>  </dd>

  <dt>Apr, 2018:</dt>
  <dd>Our dog modeling project has been covered by <A href="https://tcrn.ch/2Hcozr8">TechCrunch</A>, <A href="https://www.nbcnews.com/mach/science/why-scientists-are-teaching-ai-think-dog-ncna869266">NBC News</A>, <A href="https://www.technologyreview.com/s/610775/this-ai-thinks-like-a-dog/">MIT Technology Review</A>, <A href="https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/real-dog-behavior-could-inspire-robot-dogs">IEEE Spectrum</A>, and <A href="https://www.theverge.com/2018/4/14/17234570/artificial-intelligence-dogs-research-science-learning">The Verge</A>. </dd>



</dl>
</section>


<!-- Footer -->
<footer>
<h4>Â© Roozbeh Mottaghi</h4>
<h6>(Adopted the template from <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>'s website)</h6>
</footer>



</body></html>